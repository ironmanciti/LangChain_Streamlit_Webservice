{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TypDOiTPbjE"
   },
   "source": [
    "# LangChain ê¸°ëŠ¥ ì†Œê°œ\n",
    "\n",
    "- 1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ êµ¬ì„±  \n",
    "- 2. Runnableê³¼ LangChain í‘œí˜„ ì–¸ì–´  \n",
    "- 3. RAG ì²´ì¸ êµ¬ì¶•  \n",
    "- 4. ì²´ì¸ì´ ë‹¬ë¦° ë„êµ¬ ì‚¬ìš©  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOUiSLMxPzZY"
   },
   "source": [
    "## LangChain ê°œìš”\n",
    "\n",
    "LangChainìœ¼ë¡œ ê°œë°œëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ì¼ë°˜ì ìœ¼ë¡œ ì„¸ ê°€ì§€ ë²”ì£¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.\n",
    "\n",
    "- ì±—ë´‡ : LLMì„ ì´ìš©í•˜ì—¬ ë³´ë‹¤ ì§€ëŠ¥ì ì¸ ë§ˆì¼€íŒ…, ê³ ê° ì§€ì›, êµìœ¡ ìƒí˜¸ì‘ìš© êµ¬í˜„.\n",
    "- ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) Q&A : ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ìš”ì•½, ë°ì´í„° ë¶„ì„, ì™¸ë¶€ ì†ŒìŠ¤ë¥¼ ì°¸ì¡°í•˜ì—¬ ì½”ë“œ ìƒì„±ì— ì‚¬ìš©.\n",
    "- ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì„¤ì •ê³¼ ì¸ê°„ ìƒí˜¸ì‘ìš©ì„ í¬í•¨í•˜ë©° ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ìœ„í•´ LangGraphë¥¼ í™œìš©. ê³µê¸‰ë§ ê´€ë¦¬ ë° ìš´ì˜ ìµœì í™”ì™€ ê°™ì€ ë¶„ì•¼ì— ì ìš© ê°€ëŠ¥.\n",
    "\n",
    "LangChainì€ ìì—°ì–´ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì²´ì¸ì„ í™œìš©í•¨ìœ¼ë¡œì¨ ì‚¬ìš©ìëŠ” ìì—°ì–´ë¥¼ ì…ë ¥í•˜ê³  ë³´ê³ ì„œ, ë¶„ì„ ë˜ëŠ” ì»´í“¨í„° í”„ë¡œê·¸ë¨ê³¼ ê°™ì€ ì¶œë ¥ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNdbWOsusEgx",
    "outputId": "c8df9de3-aabf-459f-bcfa-fe09c692bfbf"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "# python-dotenv \\\n",
    "# langchain \\\n",
    "# langchain-community \\\n",
    "# openai \\\n",
    "# anthropic \\\n",
    "# langchain-openai \\\n",
    "# langchain-anthropic \\\n",
    "# langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXRwb1YcsW1j",
    "outputId": "7e5cff59-b6f5-446d-e3cc-93891014f5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env íŒŒì¼ì—ì„œ API í‚¤ë¥¼ ë¡œë“œ\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª©ë¡ ì¡°íšŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-audio-preview-2024-10-01', 'gpt-4o-realtime-preview', 'gpt-4o-realtime-preview-2024-10-01', 'dall-e-2', 'gpt-4-turbo', 'gpt-4-1106-preview', 'gpt-3.5-turbo', 'gpt-3.5-turbo-0125', 'gpt-3.5-turbo-instruct', 'babbage-002', 'whisper-1', 'dall-e-3', 'text-embedding-3-small', 'gpt-3.5-turbo-16k', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'chatgpt-4o-latest', 'omni-moderation-latest', 'gpt-4o-2024-05-13', 'o1-preview-2024-09-12', 'omni-moderation-2024-09-26', 'tts-1-hd-1106', 'o1-preview', 'gpt-4', 'gpt-4-0613', 'tts-1-hd', 'text-embedding-ada-002', 'gpt-3.5-turbo-1106', 'gpt-4o-audio-preview', 'tts-1', 'tts-1-1106', 'gpt-3.5-turbo-instruct-0914', 'davinci-002', 'text-embedding-3-large', 'gpt-4o-realtime-preview-2024-12-17', 'gpt-4o-mini-realtime-preview', 'gpt-4o-mini-realtime-preview-2024-12-17', 'o1-mini', 'gpt-4o-2024-11-20', 'o1-mini-2024-09-12', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4o-mini-audio-preview', 'gpt-4o-mini-2024-07-18', 'gpt-4o-mini', 'gpt-4o-2024-08-06', 'gpt-4o-mini-audio-preview-2024-12-17', 'gpt-4o', 'gpt-4-turbo-2024-04-09']\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "models = openai.models.list()\n",
    "\n",
    "# ê° ëª¨ë¸ì˜ ID ì¶œë ¥\n",
    "print([model.id for model in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claude-3-5-sonnet-20241022', 'claude-3-5-haiku-20241022', 'claude-3-5-sonnet-20240620', 'claude-3-haiku-20240307', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-2.1', 'claude-2.0']\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "client = anthropic.Anthropic()\n",
    "print([model.id for model in client.models.list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['models/chat-bison-001', 'models/text-bison-001', 'models/embedding-gecko-001', 'models/gemini-1.0-pro-latest', 'models/gemini-1.0-pro', 'models/gemini-pro', 'models/gemini-1.0-pro-001', 'models/gemini-1.0-pro-vision-latest', 'models/gemini-pro-vision', 'models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-001', 'models/gemini-1.5-pro-002', 'models/gemini-1.5-pro', 'models/gemini-1.5-pro-exp-0801', 'models/gemini-1.5-pro-exp-0827', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash-001', 'models/gemini-1.5-flash-001-tuning', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-exp-0827', 'models/gemini-1.5-flash-002', 'models/gemini-1.5-flash-8b', 'models/gemini-1.5-flash-8b-001', 'models/gemini-1.5-flash-8b-latest', 'models/gemini-1.5-flash-8b-exp-0827', 'models/gemini-1.5-flash-8b-exp-0924', 'models/gemini-2.0-flash-exp', 'models/gemini-exp-1206', 'models/gemini-exp-1121', 'models/gemini-exp-1114', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/learnlm-1.5-pro-experimental', 'models/embedding-001', 'models/text-embedding-004', 'models/aqa']\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "print([model.name for model in genai.list_models()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEi1gjmpW8ab"
   },
   "source": [
    "## LLM ì—°ê²°\n",
    "\n",
    "OpenAI ë° Anthropic APIì— ì—°ê²°í•©ë‹ˆë‹¤. ì›í•˜ëŠ” ëª¨ë¸ì„ ì§€ì •í•˜ì—¬ ChatOpenAI ë° ChatAnthropic í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤. llm_claude3 ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì¿¼ë¦¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì„¤ì •ì„ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jSY2uUq8ssCp"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm_gpt4 = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "llm_claude3 = ChatAnthropic(model=\"claude-3-5-haiku-20241022\")\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "NFWMvMRitGX-",
    "outputId": "9eff0799-8467-4610-ee63-d3b945657364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ìì—°ì–´ ì²˜ë¦¬(NLP)ì™€ ê´€ë ¨ëœ ê¸°ëŠ¥ì„ êµ¬ì¶•í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì£¼ë¡œ ëŒ€í™”í˜• AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ê³¼ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì–¸ì–´ ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. LangChainì€ ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:\n",
      "\n",
      "1. **ì²´ì¸(chain)**: ì—¬ëŸ¬ ê°œì˜ ì–¸ì–´ ëª¨ë¸ ë˜ëŠ” ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿**: ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í…œí”Œë¦¿ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì¼ê´€ëœ í˜•ì‹ì˜ ì‘ë‹µì„ ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ê¸°ì–µ(memory)**: ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ì—¬ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì—°ì†ì ì¸ ëŒ€í™”ë¥¼ í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "\n",
      "4. **ë°ì´í„° ì†ŒìŠ¤ í†µí•©**: ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤(ì˜ˆ: ë°ì´í„°ë² ì´ìŠ¤, API ë“±)ì™€ ì—°ê²°í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ìœ ì—°í•œ ì•„í‚¤í…ì²˜**: ê°œë°œìê°€ í•„ìš”ì— ë”°ë¼ ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ê³¼ ì•Œê³ ë¦¬ì¦˜ì„ ì‰½ê²Œ í†µí•©í•˜ê³  ì¡°ì •í•  ìˆ˜ ìˆëŠ” ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "LangChainì€ ëŒ€í™”í˜• ì‹œìŠ¤í…œ, ì±—ë´‡, ê°œì¸ ë¹„ì„œ ë“± ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ í™œìš©ë  ìˆ˜ ìˆìœ¼ë©°, ê°œë°œìë“¤ì´ ë³´ë‹¤ ì‰½ê²Œ ê³ ê¸‰ ì–¸ì–´ ì²˜ë¦¬ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(llm_gpt4.invoke(\"í•œêµ­ì–´ë¡œ LangChain ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "q9Y-0gAhxCx2",
    "outputId": "7c94fbd4-7612-4799-9ab7-6c84c7831fc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ê°„ì†Œí™”í•˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. ì£¼ìš” ê¸°ëŠ¥\n",
      "- LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ì‰¬ìš´ ê°œë°œ\n",
      "- AI ì²´ì¸ ë° ì—ì´ì „íŠ¸ ìƒì„±\n",
      "- ë‹¤ì–‘í•œ LLM ëª¨ë¸ í†µí•© ì§€ì›\n",
      "- í”„ë¡¬í”„íŠ¸ ê´€ë¦¬\n",
      "- ë©”ëª¨ë¦¬ ë° ìƒí˜¸ì‘ìš© ì¶”ì \n",
      "\n",
      "2. ì£¼ìš” êµ¬ì„± ìš”ì†Œ\n",
      "- Prompts: ì–¸ì–´ ëª¨ë¸ ì…ë ¥ ê´€ë¦¬\n",
      "- Chains: ë³µì¡í•œ ì‘ì—… ì—°ê²°\n",
      "- Agents: ììœ¨ì ì¸ ì˜ì‚¬ê²°ì • ë° ì‘ì—… ìˆ˜í–‰\n",
      "- Memory: ëŒ€í™” ìƒíƒœ ìœ ì§€\n",
      "- Tools: ì™¸ë¶€ ë¦¬ì†ŒìŠ¤ ì—°ê²°\n",
      "\n",
      "3. ì§€ì› ëª¨ë¸\n",
      "- OpenAI GPT\n",
      "- Hugging Face\n",
      "- Google Vertex AI\n",
      "- Anthropic Claude\n",
      "\n",
      "4. ì£¼ìš” í™œìš© ì‚¬ë¡€\n",
      "- ëŒ€í™”í˜• ì±—ë´‡\n",
      "- ë¬¸ì„œ ë¶„ì„\n",
      "- ë°ì´í„° ì¶”ì¶œ\n",
      "- ì½”ë“œ ìƒì„±\n",
      "- ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ\n",
      "\n",
      "Pythonì—ì„œ ì‰½ê³  ê°•ë ¥í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ì§€ì›í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(llm_claude3.invoke(\"í•œêµ­ì–´ë¡œ LangChain ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "aiKZpBOHnYJn",
    "outputId": "d2201765-56c3-49ef-a9de-3c45f9b0be74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ìš© í”„ë¡œê·¸ë¨ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.  ë‹¨ìˆœíˆ LLMì„ í˜¸ì¶œí•˜ëŠ” ê²ƒ ì´ìƒìœ¼ë¡œ, ì—¬ëŸ¬ LLMê³¼ ë‹¤ë¥¸ ìœ í˜•ì˜ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ ë”ìš± ë³µì¡í•˜ê³  ê°•ë ¥í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.  í•µì‹¬ ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "* **ëª¨ë“ˆí™”:** LangChainì€ ë‹¤ì–‘í•œ êµ¬ì„± ìš”ì†Œ(LLM, Prompts, ë©”ëª¨ë¦¬, ì¸ë±ì„œ ë“±)ë¥¼ ëª¨ë“ˆí™”í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ í™•ì¥ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.  ê° êµ¬ì„± ìš”ì†ŒëŠ” ë…ë¦½ì ìœ¼ë¡œ ê°œë°œ ë° êµì²´ë  ìˆ˜ ìˆì–´ ìœ ì—°ì„±ì´ ë›°ì–´ë‚©ë‹ˆë‹¤.\n",
      "\n",
      "* **LLM ì—°ê²°:** ë‹¤ì–‘í•œ LLM(OpenAI, Hugging Face, etc.)ì„ ì‰½ê²Œ ì—°ê²°í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  íŠ¹ì • LLMì— ì¢…ì†ë˜ì§€ ì•Šê³  í•„ìš”ì— ë”°ë¼ LLMì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "* **ë°ì´í„° ì—°ê²°:**  LLMì„ ë‹¨ìˆœíˆ ì§ˆë¬¸-ì‘ë‹µì—ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë°ì´í„°ë² ì´ìŠ¤, íŒŒì¼ ì‹œìŠ¤í…œ, ì›¹ í˜ì´ì§€ ë“± ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ ì—°ê²°í•˜ì—¬ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ì´ëŠ” LLMì˜ ì§€ì‹ ë²”ìœ„ë¥¼ í™•ì¥í•˜ê³ , ìµœì‹  ì •ë³´ë¥¼ í™œìš©í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n",
      "\n",
      "* **ë©”ëª¨ë¦¬ ê´€ë¦¬:**  ëŒ€í™”í˜• ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  í™œìš©í•˜ëŠ” ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.  ì´ëŠ” ì¼ê´€ì„± ìˆëŠ” ëŒ€í™” íë¦„ì„ ìœ ì§€í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "\n",
      "* **ì²´ì¸(Chains):** ì—¬ëŸ¬ êµ¬ì„± ìš”ì†Œë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì²´ì¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê³ , ìš”ì•½ëœ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì²´ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "* **ì—ì´ì „íŠ¸(Agents):**  ììœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ì—ì´ì „íŠ¸ëŠ” ì—¬ëŸ¬ ë„êµ¬(LLM, ê²€ìƒ‰ ì—”ì§„, ê³„ì‚°ê¸° ë“±)ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª©í‘œë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.  ì˜ˆë¥¼ ë“¤ì–´, ì—ì´ì „íŠ¸ëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ì›¹ì„ ê²€ìƒ‰í•˜ê³ , ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ìš”ì•½ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "**LangChainì„ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**\n",
      "\n",
      "* **ì±—ë´‡:**  ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” ì±—ë´‡.\n",
      "* **ìš”ì•½ ë„êµ¬:**  ê¸´ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ëŠ” ë„êµ¬.\n",
      "* **ì§ˆë¬¸-ì‘ë‹µ ì‹œìŠ¤í…œ:**  íŠ¹ì • ë°ì´í„°ì…‹ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì‹œìŠ¤í…œ.\n",
      "* **ëŒ€í™”í˜• AI ì—ì´ì „íŠ¸:**  ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì´í•´í•˜ê³ , ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸.\n",
      "* **ìë™ ë¬¸ì„œ ìƒì„±:**  íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œ.\n",
      "\n",
      "\n",
      "**ì¥ì :**\n",
      "\n",
      "* **ëª¨ë“ˆí™”ì™€ í™•ì¥ì„±:**  ë‹¤ì–‘í•œ êµ¬ì„± ìš”ì†Œë¥¼ ì¡°í•©í•˜ì—¬ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "* **ë‹¤ì–‘í•œ LLM ì§€ì›:**  ì„ í˜¸í•˜ëŠ” LLMì„ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "* **ë°ì´í„° ì—°ê²°:**  ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ë¥¼ í™œìš©í•˜ì—¬ LLMì˜ ê¸°ëŠ¥ì„ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "**ë‹¨ì :**\n",
      "\n",
      "* **ë³µì¡ì„±:**  ëª¨ë“ˆí™”ëœ êµ¬ì„± ìš”ì†Œë¥¼ ì´í•´í•˜ê³  ì¡°í•©í•˜ëŠ” ë° ì•½ê°„ì˜ í•™ìŠµ ê³¡ì„ ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "* **LLM ì˜ì¡´ì„±:**  LLMì˜ ì„±ëŠ¥ì— ë”°ë¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ì´ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "ê²°ë¡ ì ìœ¼ë¡œ LangChainì€ LLMì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•˜ê³  ê°•ë ¥í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” ë° ìœ ìš©í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.  í•˜ì§€ë§Œ ê·¸ ë³µì¡ì„±ì„ ì´í•´í•˜ê³  ì ì ˆíˆ í™œìš©í•´ì•¼ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_gemini.invoke(\"í•œêµ­ì–´ë¡œ LangChain ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages\n",
    "- AIMessage: AI(ì¸ê³µì§€ëŠ¥) ëª¨ë¸ì´ ìƒì„±í•œ ë©”ì‹œì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  \n",
    "ì˜ˆë¥¼ ë“¤ì–´, AI ëª¨ë¸ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì‘ë‹µì„ ìƒì„±í•  ë•Œ ì´ í´ë˜ìŠ¤ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "- HumanMessage: ì‚¬ëŒ(ì‚¬ìš©ì)ì´ ìƒì„±í•œ ë©”ì‹œì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  \n",
    "ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ë‚˜ ì§ˆë¬¸ ë“±ì´ ì—¬ê¸°ì— í•´ë‹¹ë©ë‹ˆë‹¤.\n",
    "\n",
    "- StemMessage: AI í–‰ë™ì„ í”„ë¼ì´ë°í•˜ê¸° ìœ„í•œ ë©”ì‹œì§€.\n",
    "ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¼ë ¨ì˜ ì…ë ¥ ë©”ì‹œì§€ ì¤‘ ì²« ë²ˆì§¸ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ëŒ€í™”ì˜ íë¦„ì„ ì œì–´í•˜ê±°ë‚˜ íŠ¹ì • ì§€ì¹¨ì„ ì œê³µí•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "** í”„ë¼ì´ë°(Priming)ì€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì—ì„œ íŠ¹ì • í–‰ë™ì´ë‚˜ ë°˜ì‘ì„ ìœ ë„í•˜ê¸° ìœ„í•´ ë¯¸ë¦¬ ì •ë³´ë¥¼ ì œê³µí•˜ê±°ë‚˜ ë§¥ë½ì„ ì„¤ì •í•˜ëŠ” ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "ë§ˆì¹˜ ë‹¤ì„¯ì‚´ ë¨¹ì€ ì–´ë¦°ì•„ì´ì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ í•œêµ­ì–´ë¡œ ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "LangChainì´ ë¬´ì—‡ì¸ê°€ìš”?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYhhibXZSGTF",
    "outputId": "3d276aa8-39a2-4b85-b9aa-5bb43729783b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChainì€ ì»´í“¨í„°ê°€ ì‚¬ëŒì²˜ëŸ¼ ë§ì„ í•˜ê³  ìƒê°í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” íŠ¹ë³„í•œ ë„êµ¬ì˜ˆìš”. ìš°ë¦¬ê°€ ì¹œêµ¬ì™€ ì´ì•¼ê¸°í•  ë•Œ, ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ëŒ€í™”í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ í•˜ëŠ” ê²ƒì²˜ëŸ¼, LangChainì€ ì»´í“¨í„°ê°€ ê·¸ëŸ° ëŒ€í™”ë¥¼ ì˜ í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ ì¤˜ìš”. \\n\\nì˜ˆë¥¼ ë“¤ì–´, LangChainì„ ì‚¬ìš©í•˜ë©´ ì»´í“¨í„°ê°€ ì±…ì„ ì½ê³ , ê·¸ ë‚´ìš©ì„ ì´í•´í•˜ê³ , ìš°ë¦¬ì—ê²Œ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ í•  ìˆ˜ ìˆê²Œ í•´ì¤˜ìš”. ë§ˆì¹˜ ì¹œêµ¬ì™€ ì´ì•¼ê¸°í•˜ëŠ” ê²ƒì²˜ëŸ¼ìš”! ê·¸ë˜ì„œ ì‚¬ëŒë“¤ì´ í•„ìš”ë¡œ í•˜ëŠ” ì •ë³´ë¥¼ ì‰½ê²Œ ì°¾ê³ , ëŒ€í™”í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì—­í• ì„ í•´ìš”.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 135, 'prompt_tokens': 43, 'total_tokens': 178, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-c613840d-b7d9-42f7-9455-ca0c619d9e1a-0', usage_metadata={'input_tokens': 43, 'output_tokens': 135, 'total_tokens': 178, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# systemê³¼ human/user ë©”ì‹œì§€ë¥¼ ì´ìš©í•œ ê¸°ë³¸ ìš”ì²­\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]\n",
    "\n",
    "response = llm_gpt4.invoke(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "AK7Xe8wnSQGa",
    "outputId": "c314481f-1ecb-4337-bf3c-08c274701b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ì»´í“¨í„°ê°€ ì‚¬ëŒì²˜ëŸ¼ ë§ì„ í•˜ê³  ìƒê°í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” íŠ¹ë³„í•œ ë„êµ¬ì˜ˆìš”. ìš°ë¦¬ê°€ ì¹œêµ¬ì™€ ì´ì•¼ê¸°í•  ë•Œ, ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ëŒ€í™”í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ í•˜ëŠ” ê²ƒì²˜ëŸ¼, LangChainì€ ì»´í“¨í„°ê°€ ê·¸ëŸ° ëŒ€í™”ë¥¼ ì˜ í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ ì¤˜ìš”. \n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´, LangChainì„ ì‚¬ìš©í•˜ë©´ ì»´í“¨í„°ê°€ ì±…ì„ ì½ê³ , ê·¸ ë‚´ìš©ì„ ì´í•´í•˜ê³ , ìš°ë¦¬ì—ê²Œ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ í•  ìˆ˜ ìˆê²Œ í•´ì¤˜ìš”. ë§ˆì¹˜ ì¹œêµ¬ì™€ ì´ì•¼ê¸°í•˜ëŠ” ê²ƒì²˜ëŸ¼ìš”! ê·¸ë˜ì„œ ì‚¬ëŒë“¤ì´ í•„ìš”ë¡œ í•˜ëŠ” ì •ë³´ë¥¼ ì‰½ê²Œ ì°¾ê³ , ëŒ€í™”í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì—­í• ì„ í•´ìš”.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4i2vpQctaFA",
    "outputId": "03c03817-31c3-4faf-df60-2f2940748bcb"
   },
   "outputs": [],
   "source": [
    "# import textwrap\n",
    "\n",
    "# #í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ë„ˆë¹„ì— ë§ê²Œ ì¤„ ë°”ê¿ˆí•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë°˜í™˜\n",
    "# answer = textwrap.fill(response.content, width=100)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtMb5t7P0tRI",
    "outputId": "003b57d1-36a4-41e1-ad93-38d0034a1d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì„ ì•„ì£¼ ì‰½ê²Œ ì„¤ëª…í•´ë³¼ê²Œìš”! ğŸ¤–\n",
      "\n",
      "LangChainì€ ë§ˆì¹˜ ë ˆê³  ë¸”ë¡ ê°™ì€ í”„ë¡œê·¸ë¨ì´ì—ìš”. \n",
      "\n",
      "ì¸ê³µì§€ëŠ¥(AI) ì–¸ì–´ ëª¨ë¸ì„ ì‰½ê³  ì¬ë¯¸ìˆê²Œ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” íŠ¹ë³„í•œ ë„êµ¬ì˜ˆìš”. ë§ˆì¹˜ ë ˆê³ ë¡œ ë‹¤ì–‘í•œ ëª¨ì–‘ì„ ë§Œë“¤ ìˆ˜ ìˆë“¯ì´, LangChainìœ¼ë¡œ ì—¬ëŸ¬ ê°€ì§€ ë©‹ì§„ AI ê¸°ëŠ¥ì„ ë§Œë“¤ ìˆ˜ ìˆë‹µë‹ˆë‹¤.\n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´:\n",
      "â€¢ ì±—ë´‡ ë§Œë“¤ê¸°\n",
      "â€¢ ë¬¸ì„œ ìš”ì•½í•˜ê¸°\n",
      "â€¢ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°\n",
      "â€¢ ë³µì¡í•œ AI ì‘ì—… ìˆ˜í–‰í•˜ê¸°\n",
      "\n",
      "ì‰½ê²Œ ë§í•´, AI í”„ë¡œê·¸ë˜ë°ì„ í›¨ì”¬ ë” ê°„ë‹¨í•˜ê³  ì¬ë¯¸ìˆê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë§ˆë²• ìƒìë¼ê³  ìƒê°í•˜ë©´ ë¼ìš”! ğŸ§™â€â™‚ï¸âœ¨\n"
     ]
    }
   ],
   "source": [
    "response = llm_claude3.invoke(message)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìˆ˜í•™ ì„ ìƒë‹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81ì„ 9ë¡œ ë‚˜ëˆ„ë©´ 9ì…ë‹ˆë‹¤. (81 Ã· 9 = 9)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"ë‹¹ì‹ ì€  ìˆ˜í•™ ë¬¸ì œë¥¼ ì˜ í’€ì–´ì£¼ëŠ” ë„ì›€ë˜ëŠ” assistant ì…ë‹ˆë‹¤.\"),\n",
    "    HumanMessage(content=\"81ì„ 9ë¡œ ë‚˜ëˆ„ë©´ ëª‡ì¸ê°€ìš”?\")\n",
    "]\n",
    "\n",
    "result = llm_gpt4.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ê³±í•˜ê¸° 5ëŠ” 50ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"ë‹¹ì‹ ì€  ìˆ˜í•™ ë¬¸ì œë¥¼ ì˜ í’€ì–´ì£¼ëŠ” ë„ì›€ë˜ëŠ” assistant ì…ë‹ˆë‹¤.\"),\n",
    "    HumanMessage(content=\"81ì„ 9ë¡œ ë‚˜ëˆ„ë©´ ëª‡ì¸ê°€ìš”?\"),\n",
    "    AIMessage(content=\"81ì„ 9ë¡œ ë‚˜ëˆ„ë©´ 9ì…ë‹ˆë‹¤.\"),\n",
    "    HumanMessage(content=\"10 ê³±í•˜ê¸° 5ëŠ” ì–¼ë§ˆì¸ê°€ìš”?\")\n",
    "]\n",
    "\n",
    "result = llm_gpt4.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Modelì„ ì´ìš©í•œ Conversation êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ì:  ì•ˆë…• ë‚´ ì´ë¦„ì€ ì˜¤ì˜ì œì•¼\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ì˜ì œë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ì:  ê°•ë‚¨ ì¼ì›ì—­ì—ì„œ ë„ë´‰ì‚°ì—­ì„ ê°€ë ¤ë©´ ì–´ë””ì„œ ê°ˆì•„íƒ€ì•¼ í•˜ë‚˜ìš”?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: ê°•ë‚¨ ì¼ì›ì—­ì—ì„œ ë„ë´‰ì‚°ì—­ìœ¼ë¡œ ê°€ì‹œë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì´ë™í•˜ì‹œë©´ ë©ë‹ˆë‹¤:\n",
      "\n",
      "1. **ì¼ì›ì—­**ì—ì„œ 3í˜¸ì„ (ìˆ˜ë„ê¶Œ ì „ì² )ìœ¼ë¡œ íƒ‘ìŠ¹í•©ë‹ˆë‹¤.\n",
      "2. 3í˜¸ì„ ì„ íƒ€ê³  **êµëŒ€ì—­**ì—ì„œ í•˜ì°¨í•©ë‹ˆë‹¤.\n",
      "3. êµëŒ€ì—­ì—ì„œ 2í˜¸ì„ ìœ¼ë¡œ ê°ˆì•„íƒ‘ë‹ˆë‹¤.\n",
      "4. 2í˜¸ì„ ì„ íƒ€ê³  **ì‹ ë¦¼ì—­**ìœ¼ë¡œ ê°€ì„œ í•˜ì°¨í•©ë‹ˆë‹¤.\n",
      "5. ì‹ ë¦¼ì—­ì—ì„œ 7í˜¸ì„ ìœ¼ë¡œ ê°ˆì•„íƒ‘ë‹ˆë‹¤.\n",
      "6. 7í˜¸ì„ ì„ íƒ€ê³  **ë„ë´‰ì‚°ì—­**ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ë°©ë²•ìœ¼ë¡œ ì´ë™í•˜ì‹œë©´ ë„ë´‰ì‚°ì—­ì— ë„ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ˆì „í•œ ì—¬í–‰ ë˜ì„¸ìš”!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ì:  ê·¸ëŸ°ë° ë‚´ ì´ë¦„ì´ ë­ì•¼?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: ì˜¤ì˜ì œë‹˜ì´ë¼ê³  í•˜ì…¨ìŠµë‹ˆë‹¤! ë§ë‚˜ìš”? ë‹¤ë¥¸ ì§ˆë¬¸ì´ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ì:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€í™” History:\n",
      "[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ì•ˆë…• ë‚´ ì´ë¦„ì€ ì˜¤ì˜ì œì•¼', additional_kwargs={}, response_metadata={}), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ì˜ì œë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={}), HumanMessage(content='ê°•ë‚¨ ì¼ì›ì—­ì—ì„œ ë„ë´‰ì‚°ì—­ì„ ê°€ë ¤ë©´ ì–´ë””ì„œ ê°ˆì•„íƒ€ì•¼ í•˜ë‚˜ìš”?', additional_kwargs={}, response_metadata={}), AIMessage(content='ê°•ë‚¨ ì¼ì›ì—­ì—ì„œ ë„ë´‰ì‚°ì—­ìœ¼ë¡œ ê°€ì‹œë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì´ë™í•˜ì‹œë©´ ë©ë‹ˆë‹¤:\\n\\n1. **ì¼ì›ì—­**ì—ì„œ 3í˜¸ì„ (ìˆ˜ë„ê¶Œ ì „ì² )ìœ¼ë¡œ íƒ‘ìŠ¹í•©ë‹ˆë‹¤.\\n2. 3í˜¸ì„ ì„ íƒ€ê³  **êµëŒ€ì—­**ì—ì„œ í•˜ì°¨í•©ë‹ˆë‹¤.\\n3. êµëŒ€ì—­ì—ì„œ 2í˜¸ì„ ìœ¼ë¡œ ê°ˆì•„íƒ‘ë‹ˆë‹¤.\\n4. 2í˜¸ì„ ì„ íƒ€ê³  **ì‹ ë¦¼ì—­**ìœ¼ë¡œ ê°€ì„œ í•˜ì°¨í•©ë‹ˆë‹¤.\\n5. ì‹ ë¦¼ì—­ì—ì„œ 7í˜¸ì„ ìœ¼ë¡œ ê°ˆì•„íƒ‘ë‹ˆë‹¤.\\n6. 7í˜¸ì„ ì„ íƒ€ê³  **ë„ë´‰ì‚°ì—­**ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.\\n\\nì´ ë°©ë²•ìœ¼ë¡œ ì´ë™í•˜ì‹œë©´ ë„ë´‰ì‚°ì—­ì— ë„ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ˆì „í•œ ì—¬í–‰ ë˜ì„¸ìš”!', additional_kwargs={}, response_metadata={}), HumanMessage(content='ê·¸ëŸ°ë° ë‚´ ì´ë¦„ì´ ë­ì•¼?', additional_kwargs={}, response_metadata={}), AIMessage(content='ì˜¤ì˜ì œë‹˜ì´ë¼ê³  í•˜ì…¨ìŠµë‹ˆë‹¤! ë§ë‚˜ìš”? ë‹¤ë¥¸ ì§ˆë¬¸ì´ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "chat_history = []\n",
    "\n",
    "# ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ì—¬ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€\n",
    "system_message = SystemMessage(content=\"You are a helpful AI assistant.\")\n",
    "chat_history.append(system_message)\n",
    "\n",
    "# ë¬´í•œ ë£¨í”„ ì‹œì‘\n",
    "while True:\n",
    "    # ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ì„ ë°›ìŒ\n",
    "    query = input(\"ì‚¬ìš©ì: \")\n",
    "    \n",
    "    # ì‚¬ìš©ìê°€ \"quit\"ë¥¼ ì…ë ¥í•˜ë©´ ë£¨í”„ë¥¼ ì¢…ë£Œ\n",
    "    if query.lower() == \"quit\":\n",
    "        break\n",
    "    \n",
    "    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "\n",
    "    # llm_gpt4 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±\n",
    "    result = llm_gpt4.invoke(chat_history)\n",
    "    response = result.content\n",
    "    \n",
    "    # AI ì‘ë‹µì„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "    \n",
    "    # AI ì‘ë‹µì„ ì¶œë ¥\n",
    "    print(f\"AI: {response}\")\n",
    "\n",
    "# ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ì¶œë ¥\n",
    "print(\"ëŒ€í™” History:\")\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fi6_NvW3MaE"
   },
   "source": [
    "##  1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ êµ¬ì„±  \n",
    "\n",
    "- LLMì—ì„œì˜ Chain ì€ Data Processing ì—ì„œì˜ Pipeline ê³¼ ìœ ì‚¬í•œ ê°œë…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G447ezl5X5ye"
   },
   "source": [
    "### ì²´ì¸ì˜ êµ¬ì„± ìš”ì†Œ - Runnables\n",
    "- í”„ë¡¬í”„íŠ¸ : LLMì˜ ì‘ë‹µì„ ì•ˆë‚´í•˜ëŠ” í…œí”Œë¦¿  \n",
    "- LLM ë˜ëŠ” ì±„íŒ… ëª¨ë¸ : í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì—”ì§„  \n",
    "- ì¶œë ¥ íŒŒì„œ : LLMì˜ ì¶œë ¥ì„ íŒŒì‹±í•˜ëŠ” ë„êµ¬  \n",
    "- ë„êµ¬ : LLMì´ APIì—ì„œ ì¶”ê°€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê±°ë‚˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ LLMì„ ì—ì´ì „íŠ¸ë¡œ ì „í™˜í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í™•ì¥ ê¸°ëŠ¥  \n",
    "- ì¼ë°˜ í•¨ìˆ˜ : ì„œë¡œ ì—°ê²°ë  ìˆ˜ ìˆëŠ” ì¶”ê°€ì ì¸ ì¼ë°˜ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Yt1j9kXR1h53"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# simple prompt template ìƒì„±\n",
    "# {topic} ë³€ìˆ˜ì—ì„œ ì‚¬ìš©ìì˜ query ê°€ ëŒ€ì²´ëœë‹¤.\n",
    "prompt_template = \"\"\"\n",
    "    ë‹¹ì‹ ì€ AI ì£¼ì œë¥¼ ì„¤ëª…í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì…ë ¥ì´ ì£¼ì–´ì§€ë©´:\n",
    "    {ì£¼ì œ}\n",
    "    ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "# prompt template ì„ ì´ìš©í•˜ì—¬ prompt ìƒì„±\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"ì£¼ì œ\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "0vikrfE53yDk",
    "outputId": "adf40012-9025-4a1a-a591-da9fd5b4883c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ìì—°ì–´ ì²˜ë¦¬(NLP)ì™€ ê´€ë ¨ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ê³¼ APIë¥¼ ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì–´ ìˆì–´, ê°œë°œìë“¤ì´ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ë³µì¡í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë³´ë‹¤ ê°„í¸í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\n",
      "\n",
      "LangChainì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ë¬¸ì„œ ê´€ë¦¬**: LangChainì€ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ê³  ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° í•„ìš”í•œ ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **í”„ë¡¬í”„íŠ¸ ê´€ë¦¬**: ì‚¬ìš©ìê°€ ì–¸ì–´ ëª¨ë¸ì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì‰½ê²Œ ìƒì„±í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì‘ë‹µ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
      "\n",
      "3. **ì²´ì¸**: LangChainì€ ì—¬ëŸ¬ ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ í”„ë¡œì„¸ìŠ¤ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆëŠ” ì²´ì¸ êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë°ì´í„° íë¦„ì„ ëª…í™•í•˜ê²Œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. **ëª¨ë¸ í†µí•©**: ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸(ì˜ˆ: OpenAIì˜ GPT, Hugging Faceì˜ ëª¨ë¸ ë“±)ê³¼ ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ê°œë°œìëŠ” íŠ¹ì • ëª¨ë¸ì˜ íŠ¹ì„±ì— ë§ì¶° ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ì‘ìš© í”„ë¡œê·¸ë¨ í…œí”Œë¦¿**: LangChainì€ ë‹¤ì–‘í•œ ìœ í˜•ì˜ NLP ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” í…œí”Œë¦¿ì„ ì œê³µí•˜ì—¬, ê°œë°œìë“¤ì´ ì‹ ì†í•˜ê²Œ í”„ë¡œí† íƒ€ì…ì„ ë§Œë“¤ê³  í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
      "\n",
      "LangChainì€ ì´ì²˜ëŸ¼ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í†µí•´ ìì—°ì–´ ì²˜ë¦¬ ê´€ë ¨ í”„ë¡œì íŠ¸ì˜ ê°œë°œ ì†ë„ë¥¼ ë†’ì´ê³ , íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°œë°œìë“¤ì€ ë”ìš± ì°½ì˜ì ì´ê³  í˜ì‹ ì ì¸ NLP ì†”ë£¨ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# pipe operator \"|\"\" ë¥¼ ì´ìš©í•˜ì—¬ í•˜ë‚˜ ì´ìƒì˜ chain ì„ ê²°í•©\n",
    "chain = prompt | llm_gpt4\n",
    "\n",
    "print(chain.invoke({\"ì£¼ì œ\": \"LangChainì´ ë­ì˜ˆìš”?\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJzXNZCnZRCr"
   },
   "source": [
    "### document loader ë¥¼ ì´ìš©í•˜ì—¬ script ìš”ì•½\n",
    "ë” ê³ ê¸‰ ì²´ì¸ì„ ë§Œë“¤ê¸° ìœ„í•´ LangChainì„ ì‚¬ìš©í•˜ì—¬ YouTube ë¹„ë””ì˜¤ë¥¼ í•„ì‚¬í•©ë‹ˆë‹¤. ë¨¼ì € YouTube Transcript APIë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ LangChainì˜ ì»¤ë®¤ë‹ˆí‹° ë¬¸ì„œ ë¡œë”ì—ì„œ YouTube Loaderë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ë¡œë”ì— ë¹„ë””ì˜¤ URLì„ ì „ë‹¬í•˜ë©´ í•„ì‚¬ë³¸ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ë¬¸ì„œ ëª©ë¡ìœ¼ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤. ì›ì‹œ í•„ì‚¬ë³¸ì„ ì–»ìœ¼ë ¤ë©´ ì´ëŸ¬í•œ ë¬¸ì„œì—ì„œ í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ê±°ë‚˜ ë¬¸ì„œë¥¼ ì²´ì¸ì— ì§ì ‘ ì „ë‹¬í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "L1HMGE6k4mQp"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "3r04ZHou46-F"
   },
   "outputs": [],
   "source": [
    "# LangChain ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì œê³µí•˜ëŠ” Youtube Loaderë¥¼ ì„í¬íŠ¸\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# YoutubeLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ íŠœë¸Œ URLì—ì„œ ë¡œë”ë¥¼ ìƒì„±\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=AOEGOhkGtjI\", add_video_info=False\n",
    ")\n",
    "\n",
    "# ë¹„ë””ì˜¤ì˜ ìë§‰ì„ ë¬¸ì„œë¡œ ë¡œë“œ\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "2Nh1VYeQ5tdt",
    "outputId": "ea0d5e86-a0bd-49f7-b7f4-0721bfedac22",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = docs[0].page_content\n",
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEScwvtSck_C"
   },
   "source": [
    "ì´ì œ, ëŒ€ë³¸ì´ ì£¼ì–´ì§„ YouTube ë¹„ë””ì˜¤ë¥¼ ìš”ì•½í•˜ëŠ” ì²´ì¸ì„ ì„¤ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ ëŒ€ë³¸ì„ ì…ë ¥ ë³€ìˆ˜ë¡œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì£¼ì…í•œ ë‹¤ìŒ íŒŒì´í”„ ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ ì„¤ì •í•©ë‹ˆë‹¤. ë¹„ë””ì˜¤ ëŒ€ë³¸ìœ¼ë¡œ í˜¸ì¶œí•˜ë©´ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ì•Šê³ ë„ ê°„ê²°í•œ ìš”ì•½ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyEU9cQo7kZF",
    "outputId": "40c8d37c-dffe-40f2-caab-0d0348e7e0e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['video_transcript'], input_types={}, partial_variables={}, template='\\n    ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ë¹„ë””ì˜¤ë¥¼ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¹„ë””ì˜¤ ëŒ€ë³¸ì´ ì£¼ì–´ì§ˆ ë•Œ:\\n    {video_transcript}\\n    í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\\n')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì „ì‚¬ëœ ë‚´ìš©ì„ chainì— ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# prompt template ìƒì„±\n",
    "prompt_template = \"\"\"\n",
    "    ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ë¹„ë””ì˜¤ë¥¼ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¹„ë””ì˜¤ ëŒ€ë³¸ì´ ì£¼ì–´ì§ˆ ë•Œ:\n",
    "    {video_transcript}\n",
    "    í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "# prompt instance ìƒì„±\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"video_transcript\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "MXvV726M8VPN"
   },
   "outputs": [],
   "source": [
    "# chain ìƒì„±\n",
    "chain = prompt | llm_gpt4\n",
    "\n",
    "answer = chain.invoke({\"video_transcript\": docs}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OW1qGhFF8e3f",
    "outputId": "d64990be-176c-4715-e60b-4331a8f208bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ë¹„ë””ì˜¤ëŠ” Metaì˜ Lama 3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ SQL ì²´ì¸ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. ë°œí‘œìëŠ” Gr Cloudì—ì„œ Lama 3ë¥¼ ì„¤ì •í•˜ê³ , SQL ìƒì„± ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ ì¡°ì • ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” e-commerce ë°ì´í„° ì„¸íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ê³ ê° ì •ë³´ ë° ìˆ˜ìµì„ ë¶„ì„í•˜ëŠ” ì—¬ëŸ¬ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì£¼ìš” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "1. **ëª¨ë¸ ì†Œê°œ**: Lama 3ëŠ” ë¹ ë¥´ê³  ê³ ê¸‰ SQL ìƒì„±ì´ ê°€ëŠ¥í•˜ë©°, ë‹¤ë¥¸ ê³ ê¸‰ LLMê³¼ ë¹„êµí•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "2. **SQL ì²´ì¸ ì„¤ì •**: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³ , ë°ì´í„° ì„¸íŠ¸ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ SQL ì²´ì¸ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
      "3. **ìê¸° ìˆ˜ì • ê¸°ëŠ¥**: SQL ì½”ë“œ ìƒì„± ì‹œ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´ë¥¼ ì²´ì¸ì— í”¼ë“œë°±í•˜ì—¬ ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
      "4. **ì¿¼ë¦¬ ì‹¤í–‰ ì˜ˆì‹œ**: ì—¬ëŸ¬ ê°€ì§€ ì¿¼ë¦¬ë¥¼ ì‹œë„í•˜ì—¬ ì„±ê³µì ì¸ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ë©°, ì˜¤ë¥˜ë¥¼ í”¼ë“œë°±í•˜ì—¬ ì„±ê³¼ë¥¼ ê°œì„ í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "5. **ë¯¸ë˜ì˜ ë°ì´í„° ë¶„ì„**: Lama 3ì™€ ê°™ì€ LLMì´ ë°ì´í„° íŒŒì´í”„ë¼ì¸, ëŒ€ì‹œë³´ë“œ ë° ë³´ê³ ì„œ ìƒì„±ì— ì¤‘ìš”í•œ ì—­í• ì„ í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, ë°ì´í„° ë¶„ì„ê°€ì™€ ì—”ì§€ë‹ˆì–´ëŠ” ì´ëŸ¬í•œ ê¸°ìˆ ì„ ìµí í•„ìš”ê°€ ìˆìŒì„ ê°•ì¡°í•©ë‹ˆë‹¤.\n",
      "\n",
      "ë¹„ë””ì˜¤ëŠ” Lama 3ì˜ ì ì¬ë ¥ê³¼ LLM ê¸°ë°˜ ë°ì´í„° ë¶„ì„ì˜ í–¥í›„ ë°©í–¥ì„±ì„ ì œì‹œí•˜ë©° ë§ˆë¬´ë¦¬ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWWyBVnI82BW"
   },
   "source": [
    "### Built-in Chains ì´ìš© ì˜ˆ\n",
    "\n",
    "- create_stuff_documents_chain : ëª¨ë¸ì— ë¬¸ì„œ ëª©ë¡ì„ ì „ë‹¬í•˜ê¸° ìœ„í•œ ì²´ì¸ì„ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "QG0H9E858fh0"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# create_stuff_documents_chainì€ ë¬¸ì„œ ëª©ë¡ì„ ë°›ì•„ ëª¨ë‘ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ í˜•ì‹í™”í•©ë‹ˆë‹¤.\n",
    "# create_stuff_documents_chain ì˜ input variable ì´ë¦„ì€ context ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ë¹„ë””ì˜¤ë¥¼ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¹„ë””ì˜¤ ëŒ€ë³¸ì´ ì£¼ì–´ì§ˆ ë•Œ:\n",
    "    {context}\n",
    "    í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "# prompt instance ìƒì„±\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# chain ìƒì„±\n",
    "chain = create_stuff_documents_chain(llm_gpt4, prompt)\n",
    "\n",
    "answer = chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "uAuC8u_--akr",
    "outputId": "ac21eafa-1390-4c7d-c289-3ebabdf1f2d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ë¹„ë””ì˜¤ëŠ” ë©”íƒ€ì˜ ìƒˆë¡œìš´ ëª¨ë¸ì¸ Lama 3ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤. Lama 3ëŠ” ë°ì´í„° ë¶„ì„ì— ìˆì–´ í° ë³€í™”ë¥¼ ê°€ì ¸ì˜¬ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë˜ë©°, íŠ¹íˆ SQL ìƒì„±ì—ì„œ ë¹ ë¥´ê³  ì •êµí•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ë¹„ë””ì˜¤ì—ì„œëŠ” SQL ì²´ì¸ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ê³¼ Lama 3ê°€ ì œê³µí•˜ëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì‚´í´ë³´ë©°, LLM ê¸°ë°˜ ë°ì´í„° ë¶„ì„ì˜ ì˜ë¯¸ì— ëŒ€í•´ì„œë„ ë…¼ì˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "ë¹„ë””ì˜¤ì—ì„œëŠ” Google Cloudì™€ LangChainì„ ì‚¬ìš©í•˜ì—¬ SQL ì²´ì¸ì„ ì„¤ì •í•˜ëŠ” ê³¼ì •ì´ ì„¤ëª…ë©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë°ì´í„°ì…‹ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí•  ê²½ìš° ì´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ëŠ” ê¸°ëŠ¥ë„ ê°•ì¡°ë˜ë©°, ì—¬ëŸ¬ ë²ˆì˜ ì‹œë„ ëì— ì„±ê³µì ì¸ ì¿¼ë¦¬ë¥¼ ì–»ëŠ” ê³¼ì •ì„ ì˜ˆì‹œë¡œ ë“¤ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "Lama 3ì˜ í™œìš© ê°€ëŠ¥ì„±ì— ëŒ€í•´ ë…¼ì˜í•˜ë©´ì„œ, ê°œì¸ ì •ë³´ ë³´í˜¸ê°€ ì¤‘ìš”í•œ ìƒí™©ì—ì„œì˜ ì‚¬ìš©ì´ë‚˜ ëŒ€ê·œëª¨ ì¿¼ë¦¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œì˜ ë¹„ìš© ë¬¸ì œ ë“±ì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ë°ì´í„° ë¶„ì„ê°€ ë° ë°ì´í„° ì—”ì§€ë‹ˆì–´ëŠ” ì´ëŸ¬í•œ ìƒˆë¡œìš´ ê¸°ìˆ ì— ì£¼ëª©í•´ì•¼ í•œë‹¤ê³  ê°•ì¡°í•˜ë©° ë¹„ë””ì˜¤ë¥¼ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Runnableê³¼ LangChain í‘œí˜„ ì–¸ì–´  \n",
    "\n",
    "- LCEL(LangChain Expression Language) ëŠ” Runnables ë¥¼ chain ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "LCELì€ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œì—ì„œ ë³µì¡í•œ ì²´ì¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ê°„ì†Œí™”í•©ë‹ˆë‹¤. íŒŒì´í”„ ì—°ì‚°ì(|)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ êµ¬ì„± ìš”ì†Œë¥¼ ì²´ì¸ìœ¼ë¡œ ì—°ê²°í•˜ê³  í•œ ìš”ì†Œì—ì„œ ë‹¤ìŒ ìš”ì†Œë¡œ ì¶œë ¥ì„ ê³µê¸‰í•©ë‹ˆë‹¤. ì´ëŸ° ë°©ì‹ìœ¼ë¡œ êµ¬ì„±ëœ ì²´ì¸ì˜ ê°„ë‹¨í•œ ì˜ˆë¡œëŠ” ëª¨ë¸ê³¼ ì¶œë ¥ íŒŒì„œê°€ ê²°í•©ëœ í”„ë¡¬í”„íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤.  ì´ëŸ¬í•œ êµ¬ì„± ìš”ì†Œë“¤ì„ runnables ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.  \n",
    "\n",
    "`chain=prompt | model | output_parser`\n",
    "\n",
    "- Chain êµ¬ì„±  \n",
    "    - ì²´ì¸ì— ëŒ€í•œ ì…ë ¥(ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ì „)\n",
    "    - ì…ë ¥ì€ í”„ë¡¬í”„íŠ¸ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.\n",
    "    - í”„ë¡¬í”„íŠ¸ ê°’ì€ LLM ë˜ëŠ” ì±„íŒ… ëª¨ë¸ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.\n",
    "    - Chatmodelì´ ì±„íŒ… ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    - íŒŒì„œëŠ” ì±„íŒ… ë©”ì‹œì§€ì—ì„œ ë¬¸ìì—´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    - ë¬¸ìì—´ì€ ì²´ì¸ì˜ ì¶œë ¥ì…ë‹ˆë‹¤\n",
    "\n",
    "- LangChainì˜ runnable ê°ì²´ë“¤:\n",
    "\n",
    "    - RunnableSequence : ì—¬ëŸ¬ runnable êµ¬ì„± ìš”ì†Œë¥¼ ì—°ê²°í•˜ì—¬ ê° êµ¬ì„± ìš”ì†Œê°€ ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³  ì¶œë ¥ì„ ë‹¤ìŒ êµ¬ì„± ìš”ì†Œì— ì „ë‹¬\n",
    "    - RunnableLambda : Pythonì˜ í˜¸ì¶œ ê°€ëŠ¥í•œ ìš”ì†Œ(í•¨ìˆ˜ ë“±)ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì„± ìš”ì†Œë¡œ ë°”ê¿”ì„œ ì²´ì¸ìœ¼ë¡œ í†µí•©\n",
    "    - RunnablePassthrough : ì…ë ¥ì„ ë³€ê²½í•˜ì§€ ì•Šê³  í†µê³¼ì‹œí‚¤ê±°ë‚˜ ì¶œë ¥ì— ì¶”ê°€ í‚¤ë¥¼ ì¶”ê°€. placeholder ì—­í• ì„ í•˜ê±°ë‚˜ ì‹œí€€ìŠ¤ì— ìœ ì—°í•˜ê²Œ í†µí•©í•  ìˆ˜ ìˆë‹¤.\n",
    "    - RunnableParallel : ì—¬ëŸ¬ ê°œì˜ ì‹¤í–‰ íŒŒì¼ì„ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ ë‘ ê°œì˜ ì²´ì¸ì´ ë™ì¼í•œ ì…ë ¥ì—ì„œ ì‹¤í–‰ë˜ì§€ë§Œ ë‹¤ë¥¸ ì¶œë ¥ì„ ë°˜í™˜í•˜ëŠ” ë¶„ê¸°ë¥¼ í—ˆìš©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "3s0DaSYO_W0Y"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "summarize_prompt_template = \"\"\"\n",
    "    ë‹¹ì‹ ì€ AI ê°œë…ì„ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤.\n",
    "    {context}\n",
    "    contextë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(summarize_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InN3aD1qGwmD"
   },
   "source": [
    "###  \"|\" ì—°ì‚°ìë¥¼ ì´ìš©í•˜ì—¬ Runnable Sequence chain ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0cahXhesl-E",
    "outputId": "eed661f4-9190-4da5-f8df-0139d0df7b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "WpJAZmpOsjHA",
    "outputId": "147f9a53-6d84-4d3c-cae6-4322635e23f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì •ë³´ ê²€ìƒ‰, ëŒ€í™”í˜• ì—ì´ì „íŠ¸, ë¬¸ì„œ ë¶„ì„ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë„êµ¬ì™€ êµ¬ì„± ìš”ì†Œë¥¼ ì œê³µí•©ë‹ˆë‹¤. LangChainì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³ , ì‚¬ìš©ìì™€ ìƒí˜¸ì‘ìš©í•˜ë©°, ë³µì¡í•œ ì–¸ì–´ ì´í•´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë° í•„ìš”í•œ ê¸°ëŠ¥ì„ í†µí•©í•˜ì—¬ ê°œë°œìë“¤ì´ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\": \"LangChain ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤˜.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_VLKGZ1HO7d"
   },
   "source": [
    "### RunnableLambda ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oI9MZ0sasxQf",
    "outputId": "c5356d5c-6811-43af-9bfb-31fe88e2638e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# RunnableLambdaë¥¼ ì‚¬ìš©í•˜ì—¬ Python í•¨ìˆ˜ë¥¼ ì²´ì¸ì— ì‚½ì…í•©ë‹ˆë‹¤.\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "print(type(summarize_chain))\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ ëŒë‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ì´ë¥¼ RunnableLambdaì— ë˜í•‘í•©ë‹ˆë‹¤.\n",
    "length_lambda = RunnableLambda(lambda summary: f\"ìš”ì•½ëœ ê¸¸ì´: {len(summary)} ê¸€ì\")\n",
    "print(type(length_lambda))\n",
    "\n",
    "lambda_chain = summarize_chain | length_lambda\n",
    "print(type(lambda_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TAa1W6iVHLnq",
    "outputId": "5b181616-a4f7-49a1-b807-9795fc389b9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìš”ì•½ëœ ê¸¸ì´: 240 ê¸€ì'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_chain.invoke({\"context\": \"LangChain ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤˜\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhIbYeOgIYyi",
    "outputId": "6ee972ca-751b-4b86-a8cd-fefe6fbe8c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context'] input_types={} partial_variables={} template='\\n    ë‹¹ì‹ ì€ AI ê°œë…ì„ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤.\\n    {context}\\n    contextë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\\n'\n",
      "client=<openai.resources.chat.completions.Completions object at 0x0000020DA72A9D90> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020DA72B5F40> root_client=<openai.OpenAI object at 0x0000020DA54632E0> root_async_client=<openai.AsyncOpenAI object at 0x0000020DA72A9DF0> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "\n",
      "RunnableLambda(lambda summary: f'ìš”ì•½ëœ ê¸¸ì´: {len(summary)} ê¸€ì')\n"
     ]
    }
   ],
   "source": [
    "for step in lambda_chain.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8q2V33ttqTg"
   },
   "source": [
    "--------------\n",
    "í•¨ìˆ˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ RunnableLambdaë¡œ ë³€í™˜í•˜ì§€ ì•Šê³ ë„ ì²´ì¸ì—ì„œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydx2GKyENIna",
    "outputId": "b53816bc-77c8-4e11-de5f-9362b79bc6ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda summary: f'ìš”ì•½ëœ ê¸¸ì´: {len(summary)} ê¸€ì')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RunnableLambdaë¡œ ë³€í™˜í•˜ì§€ ì•Šê³  ì²´ì¸ì—ì„œ í•¨ìˆ˜ ì‚¬ìš©\n",
    "chain_without_function = summarize_chain | (lambda summary: f\"ìš”ì•½ëœ ê¸¸ì´: {len(summary)} ê¸€ì\")\n",
    "\n",
    "chain_without_function.steps[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "00FqsywiNyq6",
    "outputId": "019c1f51-59e1-4b93-fc51-85d18097db15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìš”ì•½ëœ ê¸¸ì´: 244 ê¸€ì'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_without_function.invoke({\"context\": \"LangChain ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤˜\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktwM5hWrOTvQ"
   },
   "source": [
    "### placeholder ë¡œ ì‚¬ìš©ë˜ëŠ” RunnablePassthrough\n",
    "\n",
    "ì²´ì¸ ë‚´ì—ì„œ ë°ì´í„°ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³  ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì£¼ë¡œ ë””ë²„ê¹…í•˜ê±°ë‚˜ ì²´ì¸ì˜ íŠ¹ì • ë¶€ë¶„ì„ í…ŒìŠ¤íŠ¸í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cnTjJX9OOOPj",
    "outputId": "72c6fc6b-540c-4ad4-ae24-b473f9941c09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìš”ì•½ëœ ê¸¸ì´: 230 ê¸€ì'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "\n",
    "# RunnablePassthrough instance ìƒì„±\n",
    "passthrough = RunnablePassthrough()\n",
    "\n",
    "# ìš”ì•½ ë° ê¸¸ì´ ê³„ì‚°ê³¼ í•¨ê»˜ íŒŒì´í”„ ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "placeholder_chain = summarize_chain | passthrough | length_lambda\n",
    "\n",
    "placeholder_chain.invoke({\"context\": \"LangChain ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤˜\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZCrH_wbPRrw",
    "outputId": "4ee15bed-aafd-45d7-e976-785d274c6a5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    ë‹¹ì‹ ì€ AI ê°œë…ì„ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤.\\n    {context}\\n    contextë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\\n'),\n",
       " ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000020DA72A9D90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020DA72B5F40>, root_client=<openai.OpenAI object at 0x0000020DA54632E0>, root_async_client=<openai.AsyncOpenAI object at 0x0000020DA72A9DF0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')),\n",
       " StrOutputParser(),\n",
       " RunnablePassthrough(),\n",
       " RunnableLambda(lambda summary: f'ìš”ì•½ëœ ê¸¸ì´: {len(summary)} ê¸€ì')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_chain.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOnoJI94Pu0Z"
   },
   "source": [
    "### RunnablePassthrough for assignment\n",
    "- ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚¤ëŠ” ì—­í• ì„ í•˜ë©´ì„œ ì¶”ê°€ì ìœ¼ë¡œ íŠ¹ì • í‚¤ì— ê°’ì„ í• ë‹¹\n",
    "- ì²´ì¸ì„ í†µê³¼í•˜ëŠ” ë°ì´í„°ì— \"ìš”ì•½\" í•„ë“œì˜ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ê³  lengthë¼ëŠ” í‚¤ë¡œ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fb-G8U7CPY_p",
    "outputId": "14a58845-690c-4989-96b5-9e796e6c1789"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ìš”ì•½': 'LangChainì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸ê³¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ëŒ€í™”í˜• AI ì‹œìŠ¤í…œ, ì±—ë´‡, ê²€ìƒ‰ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ìš©ë„ë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LangChainì€ ë°ì´í„° ì†ŒìŠ¤ì™€ í†µí•©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ë©°, ì‚¬ìš©ì ì •ì˜ ë¡œì§ê³¼ ë‹¤ì–‘í•œ APIë¥¼ ê²°í•©í•´ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ë˜í•œ, LangChainì€ ëª¨ë“ˆí™”ëœ êµ¬ì„± ìš”ì†Œë¥¼ ì œê³µí•˜ì—¬ ê°œë°œìê°€ í•„ìš”ì— ë§ê²Œ ì‹œìŠ¤í…œì„ ì‰½ê²Œ í™•ì¥í•˜ê³  ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.',\n",
       " 'length': 276}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ìš”ì•½ì„ dictionaryë¡œ ë³€í™˜í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ ëŒë‹¤ í•¨ìˆ˜ ì •ì˜\n",
    "wrap_summary_lambda = RunnableLambda(lambda summary: {\"ìš”ì•½\": summary})\n",
    "\n",
    "# Dict ì…ë ¥ì— mapping argumentë¥¼ merge RunnablePassthrough ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "assign_passthrough = RunnablePassthrough.assign(length=lambda x: len(x['ìš”ì•½']))\n",
    "\n",
    "# summarization chain ìƒì„±\n",
    "summarize_chain = summarize_prompt | llm_gpt4 | output_parser | wrap_summary_lambda\n",
    "\n",
    "# ìš”ì•½ê³¼ assign_passthroughë¥¼ ê²°í•©í•˜ì—¬ ì „ì²´ ì²´ì¸ì„ ìƒì„±\n",
    "assign_chain = summarize_chain | assign_passthrough\n",
    "\n",
    "# Use the chain\n",
    "result = assign_chain.invoke({\"context\": \"LangChain ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤˜\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfAvpNwmxGga",
    "outputId": "f0bb779e-40d0-4369-a8e1-31c33029a6df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.passthrough.RunnableAssign'>\n"
     ]
    }
   ],
   "source": [
    "print(type(assign_chain.steps[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87QUb996RXRC"
   },
   "source": [
    "### RunnableParallel \n",
    "- ì²´ì¸ì˜ íŠ¹ì • ë‹¨ê³„ì—ì„œ ì…ë ¥ì„ ë™ì‹œì— ì—¬ëŸ¬ ì‘ì—…ì— ì „ë‹¬í•˜ê³  ê°ê°ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flTnjKw1QykS",
    "outputId": "9dd1775f-1a43-4ae2-beaf-11257781a9d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'ìš”ì•½': 'LangChainì€ ìì—°ì–´ ì²˜ë¦¬(NLP)ì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì–¸ì–´ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ì™€ í†µí•©í•˜ë©°, ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤. LangChainì€ ëª¨ë“ˆí™”ëœ êµ¬ì„± ìš”ì†Œë¥¼ ì œê³µí•˜ì—¬ ê°œë°œìê°€ ìì—°ì–´ ì²˜ë¦¬ ì‹œìŠ¤í…œì„ ì‰½ê²Œ êµ¬ì¶•í•˜ê³  í™•ì¥í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–¸ì–´ ëª¨ë¸, ë°ì´í„°ë² ì´ìŠ¤, API ë“±ì„ ì—°ê²°í•˜ì—¬ ë” í’ë¶€í•œ ê¸°ëŠ¥ì„ ê°€ì§„ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. LangChainì€ íŠ¹íˆ ëŒ€í™”í˜• AI, ì§ˆë¬¸ ì‘ë‹µ ì‹œìŠ¤í…œ, í…ìŠ¤íŠ¸ ìš”ì•½ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'},\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "#summarization chain ìƒì„±\n",
    "summarization_chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "\n",
    "# summary ì™€ ê¸¸ì´ ê³„ì‚°ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ RunnableParallel ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "parallel_runnable = RunnableParallel(\n",
    "    summary=lambda x: x,  # í˜„ì¬ì˜ summary ì „ë‹¬\n",
    "    length=lambda x: len(x)   # summaryì˜ ê¸¸ì´ ê³„ì‚°\n",
    ")\n",
    "\n",
    "# parellel runnableê³¼ summarization ê²°í•©\n",
    "parallel_chain = summarize_chain | parallel_runnable\n",
    "\n",
    "parallel_chain.invoke({\"context\": \"LangChain ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤˜\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2s-x-iRvWrY"
   },
   "source": [
    "## 3. RAG Chain ë§Œë“¤ê¸°\n",
    "\n",
    "RAGëŠ” ì™¸ë¶€ ë°ì´í„°ë¡œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì§€ì‹ì„ ì¦ê°•í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. RAG ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ë‘ ë‹¨ê³„ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:   \n",
    "   \n",
    "1) ì™¸ë¶€ ë°ì´í„° ìƒ‰ì¸í™”   \n",
    "2) ê²€ìƒ‰ ë° ì¶œë ¥ ìƒì„±.\n",
    "\n",
    "### Choma DB ì„¤ì¹˜ë¥¼ ìœ„í•´ì„œëŠ” Visual Studio Installer ì—ì„œ C++ ë¥¼ CHECK í•˜ê³  ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./db/chroma_db_test'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "# directory ê²½ë¡œ ì •ì˜\n",
    "persistent_directory = \"./db/chroma_db_test\"\n",
    "persistent_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 4335, which is longer than the specified 1000\n",
      "Created a chunk of size 5962, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Chunks Information ---\n",
      "Number of document chunks: 13\n",
      "Sample chunk:\n",
      "ëŒ€í•œë¯¼êµ­ êµ­íšŒ í™ˆí˜ì´ì§€\n",
      "\n",
      "ì˜¤ëŠ˜ì—´ì§€ì•Šê¸°\n",
      "\n",
      "ë‹«ê¸°\n",
      "\n",
      "\n",
      "ì˜¤ëŠ˜ì—´ì§€ì•Šê¸°\n",
      "\n",
      "ë‹«ê¸°\n",
      "\n",
      "\n",
      "íŒì—…ê±´ìˆ˜: ì´ 0ê±´\n",
      "\n",
      "ë‹«ê¸°\n",
      "ì˜¤ëŠ˜í•˜ë£¨ ì—´ì§€ì•Šê¸°\n",
      "\n",
      "\n",
      "ì˜¤ëŠ˜ í•˜ë£¨ ì—´ì§€ ì•Šê¸°\n",
      "\n",
      "\n",
      "ì°½ë‹«ê¸°\n",
      "\n",
      "\n",
      "ë³¸ë¬¸ë‚´ìš© ë°”ë¡œê°€ê¸°\n",
      "\n",
      "\n",
      "ì°¾ìœ¼ì‹œëŠ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”\n",
      "\n",
      "í†µí•©ê²€ìƒ‰\n",
      "\n",
      "\n",
      "ê²€ìƒ‰\n",
      "\n",
      "ì°½ë‹«ê¸°\n",
      "\n",
      "ì „ì²´ë©”ë‰´\n",
      "\n",
      "ëŒ€í•œë¯¼êµ­êµ­íšŒ\n",
      "ë‚´ì¼ì„ ì—¬ëŠ” êµ­ë¯¼ì˜ êµ­íšŒ\n",
      "\n",
      "í˜ì´ìŠ¤ë¶\n",
      "ìœ íˆ¬ë¸Œ\n",
      "íŠ¸ìœ„í„°\n",
      "ì¸ìŠ¤íƒ€ê·¸ë¨\n",
      "RSS\n",
      "\n",
      "êµ­íšŒí™œë™\n",
      "\n",
      "\n",
      "êµ­íšŒíšŒì˜\n",
      "\n",
      "\n",
      "êµ­íšŒì¼ì •\n",
      "ì˜ì‚¬ì¼ì • ê³µì§€\n",
      "ì£¼ìš”ì •ì¹˜ì¼ì • ê³µì§€\n",
      "íšŒì˜ë¡\n",
      "ì¸í„°ë„·ì˜ì‚¬ì¤‘ê³„\n",
      "ì˜ìƒíšŒì˜ë¡\n",
      "êµ­íšŒê²½ê³¼ë³´ê³ ì„œ\n",
      "\n",
      "ì˜ì•ˆ\n",
      "\n",
      "\n",
      "ì˜ì•ˆí˜„í™©\n",
      "ì˜ì•ˆì•ˆë‚´\n",
      "\n",
      "ì˜ˆì‚°ì•ˆê³¼ ê²°ì‚°\n",
      "\n",
      "\n",
      "ì˜ˆ/ê²°ì‚° í˜„í™©\n",
      "ì˜ˆ/ê²°ì‚° ì•ˆë‚´\n",
      "\n",
      "êµ­ì •ê°ì‚¬\n",
      "\n",
      "\n",
      "êµ­ì •ê°ì‚¬ í˜„í™©\n",
      "êµ­ì •ê°ì‚¬ì œë„ ì•ˆë‚´\n",
      "\n",
      "ì˜íšŒì™¸êµ\n",
      "\n",
      "\n",
      "ì˜íšŒì™¸êµê°œìš”\n",
      "ì˜íšŒì™¸êµë‹¨ì²´\n",
      "ì´ˆì²­ì™¸êµ\n",
      "ë°©ë¬¸ì™¸êµ\n",
      "êµ­ì œíšŒì˜\n",
      "\n",
      "êµ­íšŒê³µë³´\n",
      "\n",
      "\n",
      "êµ­íšŒê³µë³´\n",
      "\n",
      "ì˜ì›í™œë™\n",
      "\n",
      "\n",
      "êµ­íšŒì˜ì¥\n",
      "\n",
      "\n",
      "êµ­íšŒì˜ì¥ ì†Œê°œ\n",
      "ì£¼ìš”ì¼ì •\n",
      "ì£¼ìš”ë™ì •\n",
      "ë³´ë„ìë£Œ\n",
      "ì—°ì„¤ë¬¸\n",
      "ì—­ëŒ€ êµ­íšŒì˜ì¥\n",
      "\n",
      "êµ­íšŒë¶€ì˜ì¥\n",
      "\n",
      "\n",
      "êµ­íšŒë¶€ì˜ì¥ ì†Œê°œ\n",
      "ì£¼ìš”ì¼ì •\n",
      "ì£¼ìš”ë™ì •\n",
      "ë³´ë„ìë£Œ\n",
      "\n",
      "êµ­íšŒì˜ì›í˜„í™©\n",
      "\n",
      "\n",
      "êµ­íšŒì˜ì›í˜„í™©\n",
      "ì—­ëŒ€ êµ­íšŒì˜ì›í˜„í™©\n",
      "êµ­íšŒì˜ì› ì•ˆë‚´\n",
      "\n",
      "ì˜ì›ì‹¤í–‰ì‚¬\n",
      "\n",
      "ì˜ì›ì‹¤ì±„ìš©\n",
      "\n",
      "\n",
      "ì˜ì›ì—°êµ¬ë‹¨ì²´\n",
      "\n",
      "\n",
      "ì˜ì›ì—°êµ¬ë‹¨ì²´\n",
      "ì—°êµ¬ë‹¨ì²´ ì•ˆë‚´\n",
      "\n",
      "ì¹œì¸ì²™ë³´ì¢Œì§ì› í˜„í™©\n",
      "\n",
      "ìœ„ì›íšŒ\n",
      "\n",
      "\n",
      "ìœ„ì›íšŒí˜„í™©\n",
      "\n",
      "\n",
      "ìœ„ì›íšŒí˜„í™©\n",
      "ìœ„ì›íšŒì•ˆë‚´\n",
      "\n",
      "ìœ„ì›ëª…ë‹¨\n",
      "\n",
      "\n",
      "ìœ„ì›ëª…ë‹¨\n",
      "ìœ„ì›ëª…ë‹¨ê³µì§€\n",
      "\n",
      "ìœ„ì›íšŒì¼ì •\n",
      "\n",
      "ê³„ë¥˜ì˜ì•ˆ\n",
      "\n",
      "ìœ„ì›íšŒíšŒì˜ë¡\n",
      "\n",
      "ìœ„ì›íšŒìë£Œì‹¤\n",
      "\n",
      "ê³µì‹œì†¡ë‹¬\n",
      "\n",
      "ì†Œí†µë§ˆë‹¹\n",
      "\n",
      "\n",
      "êµ­íšŒë¯¼ì›\n",
      "\n",
      "\n",
      "êµ­íšŒë¯¼ì›\n",
      "ë¯¼ì›ì•ˆë‚´\n",
      "\n",
      "êµ­ë¯¼ë™ì˜ì²­ì›\n",
      "\n",
      "\n",
      "êµ­ë¯¼ë™ì˜ì²­ì› í˜„í™©\n",
      "êµ­ë¯¼ë™ì˜ì²­ì› ì•ˆë‚´\n",
      "\n",
      "ì…ë²•ì˜ˆê³ \n",
      "\n",
      "\n",
      "ì…ë²•ì˜ˆê³ \n",
      "ì…ë²•ì˜ˆê³  ì•ˆë‚´\n",
      "\n",
      "ì—´ë¦°êµ­íšŒì •ë³´\n",
      "\n",
      "\n",
      "ì •ë³´ê³µê°œì²­êµ¬\n",
      "\n",
      "êµ­íšŒë°•ë¬¼ê´€\n",
      "\n",
      "êµ­íšŒì°¸ê´€\n",
      "\n",
      "ë¬¸í™”í–‰ì‚¬\n",
      "\n",
      "ì˜ì •ì—°ìˆ˜\n",
      "\n",
      "\n",
      "êµ­ë¯¼ì œì•ˆ\n",
      "\n",
      "ì•Œë¦¼ë§ˆë‹¹\n",
      "\n",
      "\n",
      "ê³µì§€ì‚¬í•­\n",
      "\n",
      "ë³´ë„ìë£Œ\n",
      "\n",
      "í–‰ì‚¬ì•Œë¦¼\n",
      "\n",
      "êµ­íšŒ SNS ì†Œì‹\n",
      "\n",
      "\n",
      "êµ­íšŒê³µì‹ SNS ì†Œì‹\n",
      "êµ­íšŒì˜ì› SNS ì†Œì‹\n",
      "êµ­íšŒ SNS\n",
      "\n",
      "êµ­íšŒë§¤ê±°ì§„\n",
      "\n",
      "êµ­íšŒì •ë³´ê¸¸ë¼ì¡ì´\n",
      "\n",
      "\n",
      "ì •ì±…ì°¸ê³ ìë£Œì‹¤\n",
      "\n",
      "êµ­íšŒì±„ìš©\n",
      "\n",
      "ë¶„ì‹¤ë¬¼ ì•ˆë‚´\n",
      "\n",
      "êµ­íšŒì†Œê°œ\n",
      "\n",
      "\n",
      "êµ­íšŒì˜ êµ¬ì„±\n",
      "\n",
      "\n",
      "êµ­íšŒì˜ì¥ë‹¨\n",
      "êµ­íšŒì˜ì›\n",
      "ìœ„ì›íšŒ\n",
      "êµì„­ë‹¨ì²´\n",
      "ì…ë²•ì§€ì›ì¡°ì§\n",
      "ì¡°ì§ë„\n",
      "\n",
      "êµ­íšŒê°€ í•˜ëŠ” ì¼\n",
      "\n",
      "\n",
      "ì—­í• ê³¼ ê¶Œí•œ\n",
      "êµ­íšŒì˜ íšŒê¸°\n",
      "ë³¸íšŒì˜ ì†Œê°œ\n",
      "\n",
      "êµ­íšŒì˜ ì—­ì‚¬\n",
      "\n",
      "\n",
      "ì—­ëŒ€êµ­íšŒë³€ì²œì‚¬\n",
      "êµ­íšŒì˜ ì–´ì œì™€ ì˜¤ëŠ˜\n",
      "ì—­ëŒ€êµ­íšŒ ì†Œê°œ\n",
      "êµ­íšŒì˜ì‚¬ë‹¹ ë³€ì²œ\n",
      "\n",
      "êµ­íšŒì˜ ì´ëª¨ì €ëª¨\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: WebBaseLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ apple.comì—ì„œ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘\n",
    "# WebBaseLoaderëŠ” ì›¹ í˜ì´ì§€ë¥¼ ë¡œë“œí•˜ê³  í•´ë‹¹ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "urls = [\"https://www.assembly.go.kr/portal/main/main.do\"]\n",
    "\n",
    "# ì›¹ ì½˜í…ì¸ ì— ëŒ€í•œ ë¡œë”ë¥¼ ë§Œë“­ë‹ˆë‹¤\n",
    "loader = WebBaseLoader(urls)\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: ìŠ¤í¬ë˜í•‘ëœ ì½˜í…ì¸ ë¥¼ ì²­í¬ë¡œ ë¶„í• \n",
    "# CharacterTextSplitterëŠ” í…ìŠ¤íŠ¸ë¥¼ ë” ì‘ì€ ë©ì–´ë¦¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# ë¶„í• ëœ ë¬¸ì„œì— ëŒ€í•œ ì •ë³´ í‘œì‹œ\n",
    "print(\"\\n--- Document Chunks Information ---\")\n",
    "print(f\"Number of document chunks: {len(docs)}\")\n",
    "print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²¡í„° ì €ì¥ì†Œ ./db/chroma_db_testê°€ ì´ë¯¸ ì¡´ì¬ í•©ë‹ˆë‹¤. ìƒˆë¡œì´ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: ë¬¸ì„œ ì²­í¬ì— ëŒ€í•œ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Step 4: ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•˜ê³  ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(f\"\\n--- {persistent_directory}ì— ë²¡í„° ì €ì¥ì†Œ ìƒì„± ---\")\n",
    "    db = Chroma.from_documents(\n",
    "        docs, embeddings, persist_directory=persistent_directory)\n",
    "    print(f\"--- {persistent_directory}ì— ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ ---\")\n",
    "else:\n",
    "    print(f\"ë²¡í„° ì €ì¥ì†Œ {persistent_directory}ê°€ ì´ë¯¸ ì¡´ì¬ í•©ë‹ˆë‹¤. ìƒˆë¡œì´ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)\n",
    "\n",
    "# Step 5: ë²¡í„° ì €ì¥ì†Œ ì¿¼ë¦¬\n",
    "# ë²¡í„° ì €ì¥ì†Œë¥¼ ì¿¼ë¦¬í•˜ê¸° ìœ„í•œ ê²€ìƒ‰ê¸° ìƒì„±\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "JgOnOVNfvAnO"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"\n",
    "    ë‹¤ìŒ ë‚´ìš©ì—ë§Œ ê·¼ê±°í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "AwMZCi70AmY_"
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    # \"context\"ëŠ” ì…ë ¥ ë°ì´í„°ì—ì„œ 'question' ê°’ì„ ê°€ì ¸ì˜¤ê³ , ì´ë¥¼ retrieverì— ì „ë‹¬í•˜ì—¬ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    {\"context\": (lambda x: x['question']) | retriever,\n",
    "     \"question\": (lambda x: x['question'])}  # \"question\"ì€ ì…ë ¥ ë°ì´í„°ì—ì„œ 'question' ê°’ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "    | prompt    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì´ìš©í•´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•¨\n",
    "    | llm_gpt4   # í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPT-4 ëª¨ë¸ì„ ì‚¬ìš©í•´ ì‘ë‹µì„ ìƒì„±\n",
    "    | StrOutputParser()    # LLMì˜ ì¶œë ¥ì„ ë¬¸ìì—´ë¡œ íŒŒì‹±\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k56mgJ9Ix_Gt",
    "outputId": "6c6d5ace-7582-4d24-8d86-9f63940b27b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€í•œë¯¼êµ­ êµ­íšŒëŠ” ëŒ€êµ­ë¯¼ ì„œë¹„ìŠ¤ ì œê³µ, ë¯¼ì› ì²˜ë¦¬, ì†Œê´€ ì—…ë¬´ ìˆ˜í–‰ ë“±ì˜ ëª©ì ìœ¼ë¡œ í™œë™í•˜ë©°, ì£¼ìš” ì—­í• ì€ ë²•ë¥  ì œì •, ì˜ˆì‚° ì‹¬ì˜, êµ­ì • ê°ì‚¬, ì˜íšŒ ì™¸êµ ë“±ì…ë‹ˆë‹¤. êµ­íšŒëŠ” êµ­ë¯¼ì˜ ëŒ€í‘œë¡œì„œ ì •ì±…ì„ ë…¼ì˜í•˜ê³  ê²°ì •í•˜ëŠ” ê¸°ê´€ìœ¼ë¡œ, ì—¬ëŸ¬ ìœ„ì›íšŒì™€ ì˜ì›ì´ ì´ëŸ¬í•œ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "answer = chain.invoke({\"question\": \"ëŒ€í•œë¯¼êµ­ êµ­íšŒê°€ ì£¼ë¡œ í•˜ëŠ” ì¼ì´ ë­ì•¼?\"})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì²´ì¸ì´ ë‹¬ë¦° ë„êµ¬ ì‚¬ìš©  \n",
    "\n",
    "- ë„êµ¬ëŠ” ì—ì´ì „íŠ¸, ì²´ì¸ ë˜ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì„¸ìƒê³¼ ìƒí˜¸ ì‘ìš©í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.\n",
    "    - Python REPL, Wikipdedia, YouTube, Zapier, Gradio, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "RkuqKhVpyKxm"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade -q youtube_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZF5O2LeB-vD",
    "outputId": "86cbdc7a-8466-410b-864b-7c67acc519db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=PL48_gUxKjQ&pp=ygUP7L2U65Sp7ZWY64qUIEFJ', 'https://www.youtube.com/watch?v=-ZRP1nnjRSo&pp=ygUP7L2U65Sp7ZWY64qUIEFJ']\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "# YouTubeSearchTool ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "youtube_tool = YouTubeSearchTool()\n",
    "\n",
    "# ì•„ë¬´ í‚¤ì›Œë“œë¡œ ìœ íŠœë¸Œ ê²€ìƒ‰ ì‹¤í–‰\n",
    "results = youtube_tool.run(\"ì½”ë”©í•˜ëŠ” AI\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5BZ28LTAtul",
    "outputId": "62f913a7-6b6c-415c-d494-4ad127a6a18d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_p2jOvNM8VLDwFucfTLtEDPi8', 'function': {'arguments': '{\"query\":\"ì½”ë”©í•˜ëŠ” AI\"}', 'name': 'youtube_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 94, 'total_tokens': 112, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-96c7cf0b-1ce3-44f2-bc1e-e3f3e3850f37-0', tool_calls=[{'name': 'youtube_search', 'args': {'query': 'ì½”ë”©í•˜ëŠ” AI'}, 'id': 'call_p2jOvNM8VLDwFucfTLtEDPi8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 94, 'output_tokens': 18, 'total_tokens': 112, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YouTube ë„êµ¬ë¥¼ LLMì— ë°”ì¸ë”©í•˜ì—¬ parameter ì •ë³´ë¥¼ ì•Œì•„ëƒ„\n",
    "llm_with_tools = llm_gpt4.bind_tools([youtube_tool])\n",
    "\n",
    "msg = llm_with_tools.invoke(\"ì½”ë”©í•˜ëŠ” AI\")\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHYOhC0BCPPe",
    "outputId": "7dd07d3c-7316-4d98-849d-5e9e025e67ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'youtube_search',\n",
       "  'args': {'query': 'ì½”ë”©í•˜ëŠ” AI'},\n",
       "  'id': 'call_p2jOvNM8VLDwFucfTLtEDPi8',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_with_tools ì—ì„œ parameter ì •ë³´ë¥¼ ì•Œì•„ë‚¸ë‹¤\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'ì½”ë”©í•˜ëŠ” AI'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.tool_calls[0][\"args\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-D_-T27Azsy",
    "outputId": "87aec236-a536-48f7-8af1-bbbc8fd8f656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000020DA72A9D90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020DA72B5F40>, root_client=<openai.OpenAI object at 0x0000020DA54632E0>, root_async_client=<openai.AsyncOpenAI object at 0x0000020DA72A9DF0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'youtube_search', 'description': 'search for youtube videos associated with a person. the input to this tool should be a comma separated list, the first part contains a person name and the second a number that is the maximum number of video results to return aka num_results. the second part is optional', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| RunnableLambda(lambda x: x.tool_calls[0]['args'])\n",
       "| YouTubeSearchTool()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_with_toolsì—ì„œ ì¶”ì¶œëœ ì¸ìˆ˜ë¡œ \n",
    "# YouTubeSearchToolì„ ì‚¬ìš©í•˜ì—¬ ìœ íŠœë¸Œ ê²€ìƒ‰ ì‹¤í–‰í•œëŠ” chain ìƒì„±\n",
    "chain = llm_with_tools | (lambda x: x.tool_calls[0][\"args\"]) | youtube_tool\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"ì½”ë”©í•˜ëŠ” AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDYHTp3LIzr4",
    "outputId": "48b727f3-bd82-4602-ebdd-0a257fa76799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/watch?v=PL48_gUxKjQ&pp=ygUP7L2U65Sp7ZWY64qUIEFJ',\n",
       " 'https://www.youtube.com/watch?v=-ZRP1nnjRSo&pp=ygUP7L2U65Sp7ZWY64qUIEFJ']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "ast.literal_eval(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹¤ìŠµ : Prompt ìˆ˜ì •í•˜ì—¬ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
