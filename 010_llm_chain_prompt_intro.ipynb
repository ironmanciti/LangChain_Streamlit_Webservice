{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9316da0d",
   "metadata": {
    "id": "9316da0d"
   },
   "source": [
    "## **챗 모델과 프롬프트 템플릿을 사용해 간단한 LLM 애플리케이션 구축하기**\n",
    "\n",
    "- [**언어 모델(Language Models)**](https://docs.langchain.com/docs/concepts/chat_models) 사용법  \n",
    "- [**프롬프트 템플릿(Prompt Templates)**](https://docs.langchain.com/docs/concepts/prompt_templates) 사용법  \n",
    "- [**LangSmith**](https://docs.smith.langchain.com)를 사용한 애플리케이션 디버깅 및 추적  \n",
    "\n",
    "**LangSmith 설정**\n",
    "\n",
    "LangChain을 사용한 애플리케이션은 **여러 단계의 LLM 호출**을 포함할 수 있습니다.  \n",
    "애플리케이션이 **복잡해질수록** 내부에서 어떤 일이 일어나는지 **추적(Trace)**하는 것이 중요합니다.\n",
    "\n",
    "가장 좋은 방법은 **[LangSmith](https://smith.langchain.com)**를 사용하는 것입니다.\n",
    "\n",
    "1. **LangSmith 가입 및 설정**  \n",
    "   - 위 링크에서 가입하세요.\n",
    "<p></p>\n",
    "\n",
    "2. **환경 변수 설정**\n",
    "```\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "QqPH51oouz9r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqPH51oouz9r",
    "outputId": "88d4f7ec-8e32-4d33-c172-834d5048494f"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "# python-dotenv \\\n",
    "# langchain \\\n",
    "# langchain-community \\\n",
    "# openai \\\n",
    "# anthropic \\\n",
    "# langchain-openai \\\n",
    "# langchain-anthropic \\\n",
    "# langchain-google-genai \\\n",
    "# python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "I1APsrLauEiT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1APsrLauEiT",
    "outputId": "07352eb4-b720-42c9-cc5d-7778166cb1fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8690014-0559-44ef-afe8-eddf06c63deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain 추적(Tracing) 설정 활성화\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5558ca9",
   "metadata": {
    "id": "e5558ca9"
   },
   "source": [
    "## 언어 모델 사용하기\n",
    "\n",
    "LangChain은 다양한 언어 모델을 지원하며, 이들을 서로 교체하여 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b41234",
   "metadata": {
    "id": "e4b41234"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# GPT-4o-mini 모델을 사용하여 ChatOpenAI 인스턴스를 생성\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5642ff",
   "metadata": {
    "id": "ca5642ff"
   },
   "source": [
    "ChatModels은 LangChain Runnables의 인스턴스로, 표준화된 인터페이스를 통해 상호작용할 수 있습니다. 모델을 간단히 호출하려면 `.invoke` 메서드에 Messages 목록을 전달하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b2481f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b2481f0",
    "outputId": "4323da7b-9ebf-4ed3-f3b1-5cf182dafd16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain이란 무엇인가요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 41, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-ea6677bd-d0f1-44f3-8763-1fa62014abda-0', usage_metadata={'input_tokens': 41, 'output_tokens': 9, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# 메시지 목록을 생성\n",
    "messages = [\n",
    "    # 시스템 메시지: 모델에게 수행할 작업이나 역할을 지시합니다.\n",
    "    SystemMessage(\"다음을 영어에서 한국어로 번역하세요. 상세한 설명 말고 단순히 번역만 하세요.\"),\n",
    "    # 사용자 메시지: 사용자가 모델에 보낼 실제 입력 내용입니다.\n",
    "    HumanMessage(\"What is LangChain?\"),\n",
    "]\n",
    "\n",
    "answer = model.invoke(messages)  # `invoke` 메서드를 사용해 모델을 호출합니다.\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "CGRBL2IrZcsP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGRBL2IrZcsP",
    "outputId": "2d3464d8-e61c-4972-a3d2-c3de59493a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain이란 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83373db",
   "metadata": {
    "id": "f83373db"
   },
   "source": [
    "LangSmith를 활성화했다면 이 실행이 LangSmith에 로깅되었으며, [LangSmith 추적](https://smith.langchain.com/public/88baa0b2-7c1a-4d09-ba30-a47985dde2ea/r)을 확인할 수 있습니다. LangSmith 추적은 [토큰](/docs/concepts/tokens/) 사용 정보, 지연 시간(latency), [표준 모델 매개변수](/docs/concepts/chat_models/#standard-parameters)(예: temperature) 및 기타 정보를 제공합니다.\n",
    "\n",
    "\n",
    "ChatModel은 입력으로 [메시지](/docs/concepts/messages/) 객체를 받고 출력으로 메시지 객체를 생성합니다. 메시지 객체는 텍스트 내용 외에도 대화의 [역할](/docs/concepts/messages/#role)을 전달하며, [도구 호출](/docs/concepts/tool_calling/) 및 토큰 사용량과 같은 중요한 데이터를 포함합니다.\n",
    "\n",
    "LangChain은 문자열이나 [OpenAI 형식](/docs/concepts/messages/#openai-format)을 통해 채팅 모델 입력을 지원합니다. 다음 예제는 모두 동일한 기능을 수행합니다:\n",
    "\n",
    "```python\n",
    "model.invoke(\"Hello\")\n",
    "\n",
    "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
    "\n",
    "model.invoke([HumanMessage(\"Hello\")])\n",
    "```\n",
    "\n",
    "### 스트리밍\n",
    "\n",
    "채팅 모델은 [Runnables](/docs/concepts/runnables/)이기 때문에 비동기 및 스트리밍 호출 모드를 포함한 표준 인터페이스를 제공합니다. 이를 통해 채팅 모델로부터 개별 토큰을 스트리밍할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0abb0863-bee7-448d-b013-79d8db01e330",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0abb0863-bee7-448d-b013-79d8db01e330",
    "outputId": "93167c01-6a9c-43a9-c362-69358d3608cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Lang|Chain|은| 무엇|인가|요|?||"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8da31",
   "metadata": {
    "id": "1ab8da31"
   },
   "source": [
    "## 프롬프트 템플릿 (Prompt Templates)\n",
    "\n",
    "우리는 메시지 목록을 언어 모델에 직접 전달합니다.  일반적으로 이 목록은 사용자 입력과 애플리케이션 로직의 조합으로 구성됩니다. 애플리케이션 로직은 사용자 입력(raw user input)을 받아 언어 모델에 전달할 메시지 목록으로 변환합니다. 일반적인 변환 과정에는 시스템 메시지를 추가하거나 사용자 입력을 템플릿에 맞게 포맷하는 작업이 포함됩니다.  \n",
    "\n",
    "[프롬프트 템플릿](/docs/concepts/prompt_templates/)은 LangChain에서 이러한 변환을 돕기 위해 설계된 개념입니다. 프롬프트 템플릿은 사용자 입력(raw user input)을 받아 언어 모델에 전달할 준비가 된 데이터(프롬프트)로 반환합니다.\n",
    "\n",
    "프롬프트 템플릿은 두 개의 사용자 변수를 입력으로 받습니다:\n",
    "\n",
    "- **`language`**: 번역할 대상 언어  \n",
    "- **`text`**: 번역할 텍스트  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "I5GVnDLsdmbO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5GVnDLsdmbO",
    "outputId": "f1376bc1-fee0-49ab-9c8a-b57aa2795d4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='다음을 영어에서 {language}로 번역하세요. 상세한 설명 말고 단순히 번역만 하세요.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"다음을 영어에서 {language}로 번역하세요. 상세한 설명 말고 단순히 번역만 하세요.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [('system', system_template), ('user', '{text}')]\n",
    ")\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e876c2a",
   "metadata": {
    "id": "7e876c2a"
   },
   "source": [
    "`ChatPromptTemplate`은 하나의 템플릿에서 여러 메시지 역할을 지원합니다. `language` 매개변수는 시스템 메시지에 포맷되며, 사용자의 `text`는 사용자 메시지에 포맷됩니다.  \n",
    "이 프롬프트 템플릿의 입력은 dictionary 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f781b3cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f781b3cb",
    "outputId": "aefe4c52-daf0-402e-b6d2-6e0ba7ba448a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='다음을 영어에서 한국어로 번역하세요. 상세한 설명 말고 단순히 번역만 하세요.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"한국어\", \"text\": \"What is LangChain?\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49ba9e",
   "metadata": {
    "id": "1a49ba9e"
   },
   "source": [
    "두 개의 메시지로 구성된 `ChatPromptValue`를 반환하는 것을 볼 수 있습니다. 메시지에 직접 액세스하려면 다음을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2159b619",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2159b619",
    "outputId": "9cfb90ee-b0ef-4368-d600-bba013edf28f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='다음을 영어에서 한국어로 번역하세요. 상세한 설명 말고 단순히 번역만 하세요.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e70ee6-f0e0-4ae0-a290-002799ebf828",
   "metadata": {
    "id": "47e70ee6-f0e0-4ae0-a290-002799ebf828"
   },
   "source": [
    "마지막으로, 포맷된 프롬프트에서 채팅 모델을 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a509d8c-e122-4641-b9ee-91bc23aa155a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a509d8c-e122-4641-b9ee-91bc23aa155a",
    "outputId": "cd505eb3-4a0b-4332-8b5f-f1deb7e485c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f4790-de3a-4ec3-b54d-e5de7252db9e",
   "metadata": {},
   "source": [
    "\n",
    "프롬프트 템플릿은 사용자 입력과 매개변수를 언어 모델에 대한 지침으로 변환하는 데 도움을 줍니다. 이를 통해 모델이 컨텍스트를 이해하고 관련성 있고 일관된 언어 기반 출력을 생성하도록 유도할 수 있습니다.  \n",
    "\n",
    "프롬프트 템플릿은 **딕셔너리** 형태의 입력을 받습니다. 여기서 각 키(key)는 프롬프트 템플릿에서 채워야 할 변수를 나타냅니다.  \n",
    "\n",
    "프롬프트 템플릿은 **PromptValue**를 출력합니다. 이 **PromptValue**는 LLM 또는 ChatModel에 전달될 수 있으며, 문자열(string) 또는 메시지 목록(list of messages)으로 변환(cast)될 수도 있습니다. **PromptValue**가 존재하는 이유는 문자열과 메시지 형식 간 전환을 쉽게 하기 위함입니다.  \n",
    "\n",
    "### 프롬프트 템플릿의 유형  \n",
    "\n",
    "1. **String PromptTemplates (문자열 프롬프트 템플릿)**  \n",
    "   - 이 프롬프트 템플릿은 단일 문자열을 포맷하는 데 사용되며, 일반적으로 더 간단한 입력에 사용됩니다.  \n",
    "   - 예를 들어, 프롬프트 템플릿을 구성하고 사용하는 일반적인 방법은 다음과 같습니다:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0044d14-2834-4774-be9a-9b218d77dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고양이가 컴퓨터 앞에 앉으면 뭐라고 할까요?\n",
      "\n",
      "\"나는 마우스가 필요해!\" 😸\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"{topic}에 대한 농담을 하나 해 주세요\"\n",
    "\n",
    "# chain 작성 & invoke\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = prompt_template | model\n",
    "print(chain.invoke({\"topic\": \"고양이\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dd5d7-33c1-4717-9d25-1e3a449270fd",
   "metadata": {},
   "source": [
    "---------------\n",
    "동일한 결과를 얻는 다른 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1ed2743-b0de-4fce-b350-0070b62af3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고양이가 컴퓨터를 사용하는 이유는 뭐게요?\n",
      "\n",
      "마우스가 필요해서! 😸\n"
     ]
    }
   ],
   "source": [
    "# from_template 이용\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# prompt invoke + model invoke\n",
    "prompt = prompt_template.invoke({\"topic\": \"고양이\"})\n",
    "print(model.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cdc59-349b-45c9-8c3a-47440cbf64a8",
   "metadata": {},
   "source": [
    "2. **채팅 프롬프트 템플릿(ChatPromptTemplates)**\n",
    "    - 이 프롬프트 템플릿은 **메시지 목록을 포맷**하는 데 사용됩니다. 이러한 \"템플릿\"은 자체적으로 여러 개의 템플릿 목록으로 구성됩니다.  \n",
    "    - 예를 들어, **ChatPromptTemplate**를 구성하고 사용하는 일반적인 방법은 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1f7fd1f-5341-4112-8da6-9a89fb60118d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='당신은 도움이 되는 조수입니다', additional_kwargs={}, response_metadata={}), HumanMessage(content='고양이에 대한 농담을 하나 해 주세요', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"당신은 도움이 되는 조수입니다\"),\n",
    "    (\"user\", \"{topic}에 대한 농담을 하나 해 주세요\")\n",
    "])\n",
    "\n",
    "message = prompt_template.invoke({\"topic\": \"고양이\"})\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03cd4bc0-91f3-4a94-a67d-6ab24ac42944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜 고양이는 컴퓨터를 좋아할까요? \n",
      "\n",
      "왜냐하면 마우스가 항상 거기에 있으니까요! 😸\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(message).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314d657-cac3-41b4-a629-9af476260f4e",
   "metadata": {},
   "source": [
    "위의 예에서 ChatPromptTemplate은 호출 시 두 개의 메시지를 구성합니다. 첫 번째는 시스템 메시지로, 포맷할 변수가 없습니다. 두 번째는 HumanMessage로, 사용자가 전달하는 topic 변수로 포맷됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5af1ab-9f5b-4faa-94b4-2c6b8b13b7ad",
   "metadata": {},
   "source": [
    "## 메시지 플레이스홀더(MessagesPlaceholder)  \n",
    "\n",
    "이 프롬프트 템플릿은 특정 위치에 **메시지 목록을 추가**하는 역할을 합니다.  \n",
    "\n",
    "위의 **ChatPromptTemplate** 예제에서 두 개의 메시지를 문자열로 각각 포맷하는 방법을 확인했습니다. 하지만 특정 위치에 사용자로부터 받은 **메시지 목록**을 삽입하고 싶다면 어떻게 해야 할까요?  \n",
    "\n",
    "이럴 때 **MessagesPlaceholder**를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "715b16e2-7634-4914-a3ec-3dd2d31c1c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='당신은 도움이 되는 조수입니다', additional_kwargs={}, response_metadata={}), HumanMessage(content='고양이에 대한 농담을 하나 해 주세요', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"당신은 도움이 되는 조수입니다\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "message = prompt_template.invoke({\"msgs\": [HumanMessage(content=\"고양이에 대한 농담을 하나 해 주세요\")]})\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7c494b-b827-40dd-b8c5-901119abd2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜 고양이는 컴퓨터를 좋아할까요? \n",
      "\n",
      "왜냐하면 항상 마우스를 잡고 있으니까요! 😸\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(message).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb96ca5-086f-442c-9c8d-c2a4a30e276c",
   "metadata": {},
   "source": [
    "이렇게 하면 두 개의 메시지 목록이 생성됩니다. 첫 번째는 **시스템 메시지**이고, 두 번째는 우리가 전달한 **HumanMessage**입니다. 만약 5개의 메시지를 전달했다면, 총 6개의 메시지가 생성됩니다 (시스템 메시지 1개 + 전달된 5개 메시지).  \n",
    "\n",
    "이 방법은 특정 위치에 **메시지 목록을 삽입**할 때 유용합니다.  \n",
    "\n",
    "**MessagesPlaceholder** 클래스를 명시적으로 사용하지 않고도 동일한 작업을 수행할 수 있는 다른 방법은 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0108e8e-846c-435f-83bc-e2559eb138ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='당신은 도움이 되는 조수입니다', additional_kwargs={}, response_metadata={}), HumanMessage(content='고양이에 대한 농담을 하나 해 주세요.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"당신은 도움이 되는 조수입니다\"),\n",
    "    (\"placeholder\", \"{msgs}\") # <-- This is the changed part\n",
    "])\n",
    "\n",
    "message = prompt_template.invoke({\"msgs\": [\"고양이에 대한 농담을 하나 해 주세요.\"]})\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56b1cb01-5ce8-4f52-a67b-e09f8c8e235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜 고양이는 컴퓨터를 좋아할까요? \n",
      "\n",
      "왜냐하면 마우스를 쫓는 걸 엄청 좋아하거든요! 😸\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(message).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07473c-f9cc-418f-8aeb-468d0fb1abfd",
   "metadata": {},
   "source": [
    "## **스트리밍 (Streaming)**  \n",
    "\n",
    "**채팅 모델(Chat Models)** 은 **Runnables**이기 때문에 비동기(async) 및 **스트리밍(streaming)** 호출 모드를 포함한 표준 인터페이스를 제공합니다.  \n",
    "\n",
    "이를 통해 채팅 모델로부터 **개별 토큰(token)** 을 스트리밍 방식으로 받을 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da0c9269-5cfa-4e53-920a-f6545f169b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='다음을 영어에서 한국어로 번역하세요. 상세한 설명 말고 단순히 번역만 하세요.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3f37cc9-7eab-4d93-b658-2cf1005d4e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Lang|Chain|이|란| 무엇|인가|요|?||"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f62a58-2a42-4af4-bf6c-51015996cca3",
   "metadata": {},
   "source": [
    "**채팅 모델 응답을 스트리밍하는 방법**  \n",
    "\n",
    "모든 채팅 모델(Chat Models)은 **Runnable 인터페이스**를 구현합니다. 이 인터페이스는 표준 실행 메서드(예: `ainvoke`, `batch`, `abatch`, `stream`, `astream`, `astream_events`)의 기본 구현을 제공합니다.  \n",
    "\n",
    "기본 스트리밍 구현은 **Iterator**(또는 비동기 스트리밍의 경우 **AsyncIterator**)를 제공하며, 이는 **기본 채팅 모델 공급자**로부터 최종 출력을 단일 값으로 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c105944-9784-4274-a52d-8193bb1e65f8",
   "metadata": {},
   "source": [
    "#### **동기 스트리밍 (Sync Streaming)**  \n",
    "\n",
    "아래에서는 `|` 기호를 사용하여 토큰(token) 간의 구분자를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7c742ce-55e5-462c-ae12-c62c6e5793ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|달|빛| 아래| 계|수|나|무|,|  \n",
      "고|요|한| 밤|에| 그|늘| 드|리|워|,|  \n",
      "별|빛| 속|에| 숨|은| 꿈|들|,|  \n",
      "바|람|에| 속|삭|이는| 노|래|,|  \n",
      "우|주|를| 품|은| 고|요|한| 나|무|.||"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"달에 있는 계수나무에 대한 노래를 5줄 이내로 써주세요\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb193c-08aa-4859-8e2a-4788cb158095",
   "metadata": {},
   "source": [
    "#### **비동기 스트리밍(Async Streaming)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3897850e-e924-49c0-80fb-f80a8bfe4960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|달|빛| 아래| 계|수|나|무|,|  \n",
      "별|빛| 속|에| 춤|을| 추|네|.|  \n",
      "|바|람|에| 실|려| 오는| 노|래|,|  \n",
      "|고|요|한| 밤|에| 마음|을| 적|시|고|,|  \n",
      "달|과| 함께| 영|원|히| 빛|나|리|.||"
     ]
    }
   ],
   "source": [
    "async for chunk in model.astream(\"달에 있는 계수나무에 대한 노래를 5줄 이내로 써주세요\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e2375-a29d-4566-b48b-9d1eb70fae84",
   "metadata": {},
   "source": [
    "#### **동기/비동기 차이점 요약**\n",
    "\n",
    "| **특징**       | **Sync Streaming**           | **Async Streaming**        |\n",
    "|---------------|-------------------------------|----------------------------|\n",
    "| **실행 방식**  | 동기 (Blocking)              | 비동기 (Non-blocking)      |\n",
    "| **병렬 처리**  | 불가능                       | 가능                       |\n",
    "| **복잡성**    | 낮음                          | 높음                       |\n",
    "| **사용 사례**  | 소규모, 단순 애플리케이션     | 대규모, 고성능 애플리케이션 |\n",
    "| **예시 키워드**| `stream()`                   | `astream()`                |\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
